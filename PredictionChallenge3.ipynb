{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chandhinipj/CartPole-V0-ReinforcementLearning/blob/main/PredictionChallenge3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VKK5DA390wRe"
      },
      "source": [
        "# Skeleton Code for Prediction Challenge 3\n",
        "Below is partial code to get you started on prediction challenge 3. You need to select values for the parameters that have question marks."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install keras-rl2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DNGTghHEhyW4",
        "outputId": "7f46c161-fe6c-485f-a169-2012d9785c41"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting keras-rl2\n",
            "  Downloading keras_rl2-1.0.5-py3-none-any.whl (52 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.1/52.1 KB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tensorflow in /usr/local/lib/python3.9/dist-packages (from keras-rl2) (2.11.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (1.6.3)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (1.4.0)\n",
            "Requirement already satisfied: keras<2.12,>=2.11.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (2.11.0)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (3.19.6)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (2.2.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (3.3.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (57.4.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (3.1.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (0.2.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (23.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (15.0.6.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (0.31.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.12,>=2.11.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (2.11.0)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (1.22.4)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (1.15.0)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (0.4.0)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (23.3.3)\n",
            "Requirement already satisfied: tensorboard<2.12,>=2.11 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (2.11.2)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (4.5.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (1.51.3)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (1.15.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.9/dist-packages (from astunparse>=1.6.0->tensorflow->keras-rl2) (0.38.4)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow->keras-rl2) (0.4.6)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow->keras-rl2) (3.4.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow->keras-rl2) (2.2.3)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow->keras-rl2) (0.6.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow->keras-rl2) (2.16.2)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow->keras-rl2) (2.25.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow->keras-rl2) (1.8.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow->keras-rl2) (4.9)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow->keras-rl2) (5.3.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow->keras-rl2) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow->keras-rl2) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.9/dist-packages (from markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow->keras-rl2) (6.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow->keras-rl2) (2.10)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow->keras-rl2) (4.0.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow->keras-rl2) (1.26.14)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow->keras-rl2) (2022.12.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.9/dist-packages (from werkzeug>=1.0.1->tensorboard<2.12,>=2.11->tensorflow->keras-rl2) (2.1.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow->keras-rl2) (3.15.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.9/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow->keras-rl2) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.9/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow->keras-rl2) (3.2.2)\n",
            "Installing collected packages: keras-rl2\n",
            "Successfully installed keras-rl2-1.0.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gym"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eQ_dXOCKh0jV",
        "outputId": "49533796-5e4a-4cde-b534-855e030e7151"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.9/dist-packages (0.25.2)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.9/dist-packages (from gym) (6.0.0)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.9/dist-packages (from gym) (2.2.1)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.9/dist-packages (from gym) (1.22.4)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.9/dist-packages (from gym) (0.0.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=4.8.0->gym) (3.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load the gym module\n",
        "import gym\n",
        "import matplotlib.pyplot as plt\n",
        "# import the usual Keras modules for creating deep neural networks\n",
        "from keras import Sequential\n",
        "from keras.layers import Input, Flatten, Dense\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "ENV_NAME = 'CartPole-v0'\n",
        "env = gym.make(ENV_NAME)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "75Phf54nh2y_",
        "outputId": "98d6ac63-7738-4727-ed92-fc54cab0dbcb"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/gym/envs/registration.py:593: UserWarning: \u001b[33mWARN: The environment CartPole-v0 is out of date. You should consider upgrading to version `v1`.\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.9/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import rl\n",
        "from rl.memory import SequentialMemory  # import the exerience replay buffer module\n",
        "from rl.policy import LinearAnnealedPolicy, EpsGreedyQPolicy  # import the policy\n",
        "from rl.agents.dqn import DQNAgent      # import the DQN agent"
      ],
      "metadata": {
        "id": "KsGYDOrDh58e"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q-Network\n",
        "model = Sequential()\n",
        "model.add(Input(shape=(1,env.observation_space.shape[0])))  # The input is 1 observation vector, and the number of observations in that vector \n",
        "model.add(Flatten())\n",
        "# add extra layers here\n",
        "model.add(Dense(16, activation='relu'))\n",
        "model.add(Dense(env.action_space.n, activation='linear'))   # the output is the number of actions in the action space\n",
        "print(model.summary())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i5dvD8xTio0U",
        "outputId": "c5696608-d002-4cce-d042-73d67c3dec8c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten (Flatten)           (None, 4)                 0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 16)                80        \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 2)                 34        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 114\n",
            "Trainable params: 114\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R8ZiiRbxlH2D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "55161608-9c12-4e90-9f7c-34ebaf1657fc"
      },
      "source": [
        "memory = SequentialMemory(limit=10000, window_length=1)\n",
        "\n",
        "# define the policy\n",
        "policy =  LinearAnnealedPolicy(inner_policy=EpsGreedyQPolicy(), \n",
        "                               attr='eps',            \n",
        "                               value_max=1.0,\n",
        "                               value_min=0.1, \n",
        "                               value_test=.05,\n",
        "                               nb_steps=10000)\n",
        "\n",
        "\n",
        "# define the agent\n",
        "dqn = DQNAgent(model=model, \n",
        "               nb_actions=env.action_space.n,\n",
        "               memory=memory,\n",
        "               nb_steps_warmup=20,\n",
        "               target_model_update=1000, \n",
        "               policy=policy) \n",
        "\n",
        "dqn.compile(Adam(lr=0.001), metrics=['mae'])\n",
        "\n",
        "history = dqn.fit(env, nb_steps=10000, visualize=False, verbose=2)\n",
        "\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/optimizers/optimizer_v2/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.9/dist-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
            "Instructions for updating:\n",
            "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n",
            "/usr/local/lib/python3.9/dist-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates=self.state_updates,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training for 10000 steps ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/rl/memory.py:37: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
            "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n",
            "/usr/local/lib/python3.9/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 20 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
            "/usr/local/lib/python3.9/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 21 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
            "/usr/local/lib/python3.9/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 22 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
            "/usr/local/lib/python3.9/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 23 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
            "/usr/local/lib/python3.9/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 24 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
            "/usr/local/lib/python3.9/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 25 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
            "/usr/local/lib/python3.9/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 26 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
            "/usr/local/lib/python3.9/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 27 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
            "/usr/local/lib/python3.9/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 28 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
            "/usr/local/lib/python3.9/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 29 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
            "/usr/local/lib/python3.9/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 30 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
            "/usr/local/lib/python3.9/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 31 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   33/10000: episode: 1, duration: 1.529s, episode steps:  33, steps per second:  22, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.455 [0.000, 1.000],  loss: 0.489943, mae: 0.552441, mean_q: -0.048480, mean_eps: 0.997615\n",
            "   92/10000: episode: 2, duration: 0.672s, episode steps:  59, steps per second:  88, episode reward: 59.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.475 [0.000, 1.000],  loss: 0.310318, mae: 0.464620, mean_q: 0.225458, mean_eps: 0.994420\n",
            "  104/10000: episode: 3, duration: 0.118s, episode steps:  12, steps per second: 102, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 0.198550, mae: 0.443566, mean_q: 0.416144, mean_eps: 0.991225\n",
            "  129/10000: episode: 4, duration: 0.283s, episode steps:  25, steps per second:  88, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  loss: 0.155486, mae: 0.451079, mean_q: 0.518494, mean_eps: 0.989560\n",
            "  153/10000: episode: 5, duration: 0.384s, episode steps:  24, steps per second:  62, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.542 [0.000, 1.000],  loss: 0.112802, mae: 0.460078, mean_q: 0.616380, mean_eps: 0.987355\n",
            "  180/10000: episode: 6, duration: 0.304s, episode steps:  27, steps per second:  89, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.444 [0.000, 1.000],  loss: 0.083005, mae: 0.460346, mean_q: 0.694890, mean_eps: 0.985060\n",
            "  193/10000: episode: 7, duration: 0.139s, episode steps:  13, steps per second:  94, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: 0.061235, mae: 0.466369, mean_q: 0.745722, mean_eps: 0.983260\n",
            "  214/10000: episode: 8, duration: 0.227s, episode steps:  21, steps per second:  92, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.429 [0.000, 1.000],  loss: 0.054304, mae: 0.468470, mean_q: 0.763690, mean_eps: 0.981730\n",
            "  238/10000: episode: 9, duration: 0.256s, episode steps:  24, steps per second:  94, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.542 [0.000, 1.000],  loss: 0.041574, mae: 0.466997, mean_q: 0.788546, mean_eps: 0.979705\n",
            "  261/10000: episode: 10, duration: 0.385s, episode steps:  23, steps per second:  60, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.565 [0.000, 1.000],  loss: 0.034583, mae: 0.465354, mean_q: 0.797915, mean_eps: 0.977590\n",
            "  282/10000: episode: 11, duration: 0.252s, episode steps:  21, steps per second:  83, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 0.029242, mae: 0.471655, mean_q: 0.832414, mean_eps: 0.975610\n",
            "  309/10000: episode: 12, duration: 0.274s, episode steps:  27, steps per second:  99, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.593 [0.000, 1.000],  loss: 0.021527, mae: 0.478662, mean_q: 0.861973, mean_eps: 0.973450\n",
            "  322/10000: episode: 13, duration: 0.233s, episode steps:  13, steps per second:  56, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.385 [0.000, 1.000],  loss: 0.015549, mae: 0.484064, mean_q: 0.895235, mean_eps: 0.971650\n",
            "  353/10000: episode: 14, duration: 0.388s, episode steps:  31, steps per second:  80, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.484 [0.000, 1.000],  loss: 0.012748, mae: 0.487854, mean_q: 0.919741, mean_eps: 0.969670\n",
            "  399/10000: episode: 15, duration: 0.615s, episode steps:  46, steps per second:  75, episode reward: 46.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.478 [0.000, 1.000],  loss: 0.011404, mae: 0.492151, mean_q: 0.923224, mean_eps: 0.966205\n",
            "  416/10000: episode: 16, duration: 0.169s, episode steps:  17, steps per second: 100, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.294 [0.000, 1.000],  loss: 0.013845, mae: 0.490375, mean_q: 0.918162, mean_eps: 0.963370\n",
            "  439/10000: episode: 17, duration: 0.366s, episode steps:  23, steps per second:  63, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.478 [0.000, 1.000],  loss: 0.014085, mae: 0.499549, mean_q: 0.940720, mean_eps: 0.961570\n",
            "  450/10000: episode: 18, duration: 0.126s, episode steps:  11, steps per second:  87, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.364 [0.000, 1.000],  loss: 0.010879, mae: 0.485889, mean_q: 0.930592, mean_eps: 0.960040\n",
            "  480/10000: episode: 19, duration: 0.534s, episode steps:  30, steps per second:  56, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 0.008851, mae: 0.482094, mean_q: 0.927934, mean_eps: 0.958195\n",
            "  499/10000: episode: 20, duration: 0.571s, episode steps:  19, steps per second:  33, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  loss: 0.009293, mae: 0.486528, mean_q: 0.932315, mean_eps: 0.955990\n",
            "  526/10000: episode: 21, duration: 0.669s, episode steps:  27, steps per second:  40, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.444 [0.000, 1.000],  loss: 0.007461, mae: 0.477607, mean_q: 0.936726, mean_eps: 0.953920\n",
            "  538/10000: episode: 22, duration: 0.299s, episode steps:  12, steps per second:  40, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.250 [0.000, 1.000],  loss: 0.007198, mae: 0.480872, mean_q: 0.937589, mean_eps: 0.952165\n",
            "  578/10000: episode: 23, duration: 0.980s, episode steps:  40, steps per second:  41, episode reward: 40.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.450 [0.000, 1.000],  loss: 0.006246, mae: 0.475042, mean_q: 0.932845, mean_eps: 0.949825\n",
            "  607/10000: episode: 24, duration: 0.338s, episode steps:  29, steps per second:  86, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.586 [0.000, 1.000],  loss: 0.005400, mae: 0.473785, mean_q: 0.935129, mean_eps: 0.946720\n",
            "  618/10000: episode: 25, duration: 0.103s, episode steps:  11, steps per second: 106, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.727 [0.000, 1.000],  loss: 0.006926, mae: 0.484163, mean_q: 0.951117, mean_eps: 0.944920\n",
            "  634/10000: episode: 26, duration: 0.236s, episode steps:  16, steps per second:  68, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.625 [0.000, 1.000],  loss: 0.005472, mae: 0.480313, mean_q: 0.942913, mean_eps: 0.943705\n",
            "  664/10000: episode: 27, duration: 0.339s, episode steps:  30, steps per second:  89, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  loss: 0.005419, mae: 0.476715, mean_q: 0.938451, mean_eps: 0.941635\n",
            "  680/10000: episode: 28, duration: 0.160s, episode steps:  16, steps per second: 100, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.625 [0.000, 1.000],  loss: 0.004781, mae: 0.481685, mean_q: 0.951643, mean_eps: 0.939565\n",
            "  701/10000: episode: 29, duration: 0.257s, episode steps:  21, steps per second:  82, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  loss: 0.005307, mae: 0.479094, mean_q: 0.948585, mean_eps: 0.937900\n",
            "  734/10000: episode: 30, duration: 0.354s, episode steps:  33, steps per second:  93, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.455 [0.000, 1.000],  loss: 0.004772, mae: 0.478033, mean_q: 0.948219, mean_eps: 0.935470\n",
            "  748/10000: episode: 31, duration: 0.160s, episode steps:  14, steps per second:  88, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.643 [0.000, 1.000],  loss: 0.004403, mae: 0.479345, mean_q: 0.948163, mean_eps: 0.933355\n",
            "  763/10000: episode: 32, duration: 0.316s, episode steps:  15, steps per second:  47, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 0.003462, mae: 0.476090, mean_q: 0.947963, mean_eps: 0.932050\n",
            "  777/10000: episode: 33, duration: 0.268s, episode steps:  14, steps per second:  52, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.643 [0.000, 1.000],  loss: 0.004517, mae: 0.474826, mean_q: 0.939656, mean_eps: 0.930745\n",
            "  798/10000: episode: 34, duration: 0.275s, episode steps:  21, steps per second:  76, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.429 [0.000, 1.000],  loss: 0.004149, mae: 0.474365, mean_q: 0.948563, mean_eps: 0.929170\n",
            "  811/10000: episode: 35, duration: 0.182s, episode steps:  13, steps per second:  71, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.692 [0.000, 1.000],  loss: 0.002943, mae: 0.470378, mean_q: 0.942253, mean_eps: 0.927640\n",
            "  828/10000: episode: 36, duration: 0.201s, episode steps:  17, steps per second:  85, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.647 [0.000, 1.000],  loss: 0.003176, mae: 0.468037, mean_q: 0.940390, mean_eps: 0.926290\n",
            "  844/10000: episode: 37, duration: 0.188s, episode steps:  16, steps per second:  85, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.625 [0.000, 1.000],  loss: 0.003257, mae: 0.473836, mean_q: 0.951511, mean_eps: 0.924805\n",
            "  909/10000: episode: 38, duration: 0.847s, episode steps:  65, steps per second:  77, episode reward: 65.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.508 [0.000, 1.000],  loss: 0.002749, mae: 0.470850, mean_q: 0.948696, mean_eps: 0.921160\n",
            "  930/10000: episode: 39, duration: 0.244s, episode steps:  21, steps per second:  86, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.381 [0.000, 1.000],  loss: 0.002843, mae: 0.470734, mean_q: 0.947904, mean_eps: 0.917290\n",
            "  964/10000: episode: 40, duration: 0.350s, episode steps:  34, steps per second:  97, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  loss: 0.002530, mae: 0.467598, mean_q: 0.947624, mean_eps: 0.914815\n",
            "  982/10000: episode: 41, duration: 0.190s, episode steps:  18, steps per second:  95, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.444 [0.000, 1.000],  loss: 0.002484, mae: 0.471429, mean_q: 0.950834, mean_eps: 0.912475\n",
            " 1001/10000: episode: 42, duration: 0.219s, episode steps:  19, steps per second:  87, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.632 [0.000, 1.000],  loss: 0.002339, mae: 0.467650, mean_q: 0.949684, mean_eps: 0.910810\n",
            " 1072/10000: episode: 43, duration: 0.905s, episode steps:  71, steps per second:  78, episode reward: 71.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.493 [0.000, 1.000],  loss: 0.136225, mae: 0.975184, mean_q: 1.633244, mean_eps: 0.906760\n",
            " 1096/10000: episode: 44, duration: 0.280s, episode steps:  24, steps per second:  86, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.417 [0.000, 1.000],  loss: 0.018367, mae: 0.998920, mean_q: 1.939393, mean_eps: 0.902485\n",
            " 1152/10000: episode: 45, duration: 0.354s, episode steps:  56, steps per second: 158, episode reward: 56.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.554 [0.000, 1.000],  loss: 0.018425, mae: 0.995614, mean_q: 1.939851, mean_eps: 0.898885\n",
            " 1180/10000: episode: 46, duration: 0.188s, episode steps:  28, steps per second: 149, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.357 [0.000, 1.000],  loss: 0.016729, mae: 0.992845, mean_q: 1.933125, mean_eps: 0.895105\n",
            " 1198/10000: episode: 47, duration: 0.122s, episode steps:  18, steps per second: 147, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.389 [0.000, 1.000],  loss: 0.014629, mae: 0.986313, mean_q: 1.930075, mean_eps: 0.893035\n",
            " 1210/10000: episode: 48, duration: 0.081s, episode steps:  12, steps per second: 148, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.750 [0.000, 1.000],  loss: 0.012965, mae: 0.987882, mean_q: 1.938947, mean_eps: 0.891685\n",
            " 1235/10000: episode: 49, duration: 0.164s, episode steps:  25, steps per second: 152, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.360 [0.000, 1.000],  loss: 0.017319, mae: 0.992320, mean_q: 1.935185, mean_eps: 0.890020\n",
            " 1251/10000: episode: 50, duration: 0.103s, episode steps:  16, steps per second: 156, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.688 [0.000, 1.000],  loss: 0.018057, mae: 0.990377, mean_q: 1.925863, mean_eps: 0.888175\n",
            " 1293/10000: episode: 51, duration: 0.251s, episode steps:  42, steps per second: 167, episode reward: 42.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.619 [0.000, 1.000],  loss: 0.014191, mae: 0.988411, mean_q: 1.925336, mean_eps: 0.885565\n",
            " 1309/10000: episode: 52, duration: 0.096s, episode steps:  16, steps per second: 166, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.438 [0.000, 1.000],  loss: 0.014032, mae: 0.983162, mean_q: 1.920366, mean_eps: 0.882955\n",
            " 1345/10000: episode: 53, duration: 0.231s, episode steps:  36, steps per second: 156, episode reward: 36.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.528 [0.000, 1.000],  loss: 0.014871, mae: 0.991981, mean_q: 1.942187, mean_eps: 0.880615\n",
            " 1355/10000: episode: 54, duration: 0.070s, episode steps:  10, steps per second: 143, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.300 [0.000, 1.000],  loss: 0.011190, mae: 0.988370, mean_q: 1.940967, mean_eps: 0.878545\n",
            " 1417/10000: episode: 55, duration: 0.414s, episode steps:  62, steps per second: 150, episode reward: 62.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.484 [0.000, 1.000],  loss: 0.015723, mae: 0.990479, mean_q: 1.936864, mean_eps: 0.875305\n",
            " 1442/10000: episode: 56, duration: 0.159s, episode steps:  25, steps per second: 158, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  loss: 0.014351, mae: 0.987556, mean_q: 1.931157, mean_eps: 0.871390\n",
            " 1478/10000: episode: 57, duration: 0.231s, episode steps:  36, steps per second: 156, episode reward: 36.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.015113, mae: 0.986015, mean_q: 1.936283, mean_eps: 0.868645\n",
            " 1494/10000: episode: 58, duration: 0.112s, episode steps:  16, steps per second: 142, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.562 [0.000, 1.000],  loss: 0.008431, mae: 0.982906, mean_q: 1.953423, mean_eps: 0.866305\n",
            " 1548/10000: episode: 59, duration: 0.331s, episode steps:  54, steps per second: 163, episode reward: 54.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  loss: 0.012542, mae: 0.986774, mean_q: 1.940685, mean_eps: 0.863155\n",
            " 1601/10000: episode: 60, duration: 0.308s, episode steps:  53, steps per second: 172, episode reward: 53.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.491 [0.000, 1.000],  loss: 0.013183, mae: 0.986487, mean_q: 1.941491, mean_eps: 0.858340\n",
            " 1626/10000: episode: 61, duration: 0.143s, episode steps:  25, steps per second: 175, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  loss: 0.011428, mae: 0.986601, mean_q: 1.937743, mean_eps: 0.854830\n",
            " 1649/10000: episode: 62, duration: 0.151s, episode steps:  23, steps per second: 152, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  loss: 0.015018, mae: 0.989275, mean_q: 1.934025, mean_eps: 0.852670\n",
            " 1664/10000: episode: 63, duration: 0.125s, episode steps:  15, steps per second: 120, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 0.013890, mae: 0.985392, mean_q: 1.934759, mean_eps: 0.850960\n",
            " 1719/10000: episode: 64, duration: 0.491s, episode steps:  55, steps per second: 112, episode reward: 55.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.455 [0.000, 1.000],  loss: 0.012264, mae: 0.986194, mean_q: 1.938702, mean_eps: 0.847810\n",
            " 1765/10000: episode: 65, duration: 0.420s, episode steps:  46, steps per second: 110, episode reward: 46.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.543 [0.000, 1.000],  loss: 0.012565, mae: 0.987069, mean_q: 1.939221, mean_eps: 0.843265\n",
            " 1787/10000: episode: 66, duration: 0.226s, episode steps:  22, steps per second:  98, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.455 [0.000, 1.000],  loss: 0.010201, mae: 0.987960, mean_q: 1.946304, mean_eps: 0.840205\n",
            " 1815/10000: episode: 67, duration: 0.243s, episode steps:  28, steps per second: 115, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.464 [0.000, 1.000],  loss: 0.011065, mae: 0.987027, mean_q: 1.948501, mean_eps: 0.837955\n",
            " 1826/10000: episode: 68, duration: 0.110s, episode steps:  11, steps per second: 100, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.091 [0.000, 1.000],  loss: 0.012785, mae: 0.984988, mean_q: 1.948690, mean_eps: 0.836200\n",
            " 1877/10000: episode: 69, duration: 0.463s, episode steps:  51, steps per second: 110, episode reward: 51.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 0.013135, mae: 0.985429, mean_q: 1.942990, mean_eps: 0.833410\n",
            " 1902/10000: episode: 70, duration: 0.242s, episode steps:  25, steps per second: 103, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.440 [0.000, 1.000],  loss: 0.015269, mae: 0.983998, mean_q: 1.931623, mean_eps: 0.829990\n",
            " 1912/10000: episode: 71, duration: 0.098s, episode steps:  10, steps per second: 102, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.700 [0.000, 1.000],  loss: 0.009267, mae: 0.985371, mean_q: 1.941268, mean_eps: 0.828415\n",
            " 1939/10000: episode: 72, duration: 0.201s, episode steps:  27, steps per second: 134, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.407 [0.000, 1.000],  loss: 0.014210, mae: 0.985474, mean_q: 1.938308, mean_eps: 0.826750\n",
            " 1964/10000: episode: 73, duration: 0.149s, episode steps:  25, steps per second: 168, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  loss: 0.010857, mae: 0.983340, mean_q: 1.941232, mean_eps: 0.824410\n",
            " 1982/10000: episode: 74, duration: 0.125s, episode steps:  18, steps per second: 145, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 0.015246, mae: 0.987433, mean_q: 1.945619, mean_eps: 0.822475\n",
            " 1999/10000: episode: 75, duration: 0.118s, episode steps:  17, steps per second: 144, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  loss: 0.009435, mae: 0.982514, mean_q: 1.943827, mean_eps: 0.820900\n",
            " 2009/10000: episode: 76, duration: 0.076s, episode steps:  10, steps per second: 132, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.300 [0.000, 1.000],  loss: 0.351109, mae: 1.359954, mean_q: 2.016859, mean_eps: 0.819685\n",
            " 2034/10000: episode: 77, duration: 0.170s, episode steps:  25, steps per second: 147, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.440 [0.000, 1.000],  loss: 0.119561, mae: 1.491374, mean_q: 2.665457, mean_eps: 0.818110\n",
            " 2127/10000: episode: 78, duration: 0.587s, episode steps:  93, steps per second: 158, episode reward: 93.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  loss: 0.057826, mae: 1.498638, mean_q: 2.901820, mean_eps: 0.812800\n",
            " 2141/10000: episode: 79, duration: 0.094s, episode steps:  14, steps per second: 149, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.643 [0.000, 1.000],  loss: 0.064222, mae: 1.497784, mean_q: 2.882618, mean_eps: 0.807985\n",
            " 2154/10000: episode: 80, duration: 0.097s, episode steps:  13, steps per second: 135, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.154 [0.000, 1.000],  loss: 0.050875, mae: 1.486154, mean_q: 2.869409, mean_eps: 0.806770\n",
            " 2180/10000: episode: 81, duration: 0.177s, episode steps:  26, steps per second: 147, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.423 [0.000, 1.000],  loss: 0.040248, mae: 1.491679, mean_q: 2.908874, mean_eps: 0.805015\n",
            " 2193/10000: episode: 82, duration: 0.085s, episode steps:  13, steps per second: 153, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.231 [0.000, 1.000],  loss: 0.042203, mae: 1.487109, mean_q: 2.909732, mean_eps: 0.803260\n",
            " 2240/10000: episode: 83, duration: 0.295s, episode steps:  47, steps per second: 159, episode reward: 47.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.511 [0.000, 1.000],  loss: 0.045133, mae: 1.487102, mean_q: 2.894158, mean_eps: 0.800560\n",
            " 2272/10000: episode: 84, duration: 0.234s, episode steps:  32, steps per second: 137, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.469 [0.000, 1.000],  loss: 0.054089, mae: 1.489170, mean_q: 2.870013, mean_eps: 0.797005\n",
            " 2309/10000: episode: 85, duration: 0.252s, episode steps:  37, steps per second: 147, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.514 [0.000, 1.000],  loss: 0.052027, mae: 1.498602, mean_q: 2.901652, mean_eps: 0.793900\n",
            " 2332/10000: episode: 86, duration: 0.151s, episode steps:  23, steps per second: 153, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  loss: 0.059632, mae: 1.493062, mean_q: 2.870118, mean_eps: 0.791200\n",
            " 2347/10000: episode: 87, duration: 0.104s, episode steps:  15, steps per second: 145, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  loss: 0.051673, mae: 1.488871, mean_q: 2.889079, mean_eps: 0.789490\n",
            " 2378/10000: episode: 88, duration: 0.195s, episode steps:  31, steps per second: 159, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.484 [0.000, 1.000],  loss: 0.048953, mae: 1.490243, mean_q: 2.900547, mean_eps: 0.787420\n",
            " 2466/10000: episode: 89, duration: 0.568s, episode steps:  88, steps per second: 155, episode reward: 88.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.466 [0.000, 1.000],  loss: 0.050789, mae: 1.495925, mean_q: 2.905387, mean_eps: 0.782065\n",
            " 2486/10000: episode: 90, duration: 0.128s, episode steps:  20, steps per second: 156, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.450 [0.000, 1.000],  loss: 0.051067, mae: 1.493998, mean_q: 2.901761, mean_eps: 0.777205\n",
            " 2510/10000: episode: 91, duration: 0.166s, episode steps:  24, steps per second: 145, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.051969, mae: 1.494926, mean_q: 2.907930, mean_eps: 0.775225\n",
            " 2558/10000: episode: 92, duration: 0.303s, episode steps:  48, steps per second: 158, episode reward: 48.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.040836, mae: 1.490062, mean_q: 2.903649, mean_eps: 0.771985\n",
            " 2648/10000: episode: 93, duration: 0.585s, episode steps:  90, steps per second: 154, episode reward: 90.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.489 [0.000, 1.000],  loss: 0.050286, mae: 1.489651, mean_q: 2.882987, mean_eps: 0.765775\n",
            " 2741/10000: episode: 94, duration: 0.586s, episode steps:  93, steps per second: 159, episode reward: 93.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  loss: 0.042940, mae: 1.488842, mean_q: 2.899083, mean_eps: 0.757540\n",
            " 2771/10000: episode: 95, duration: 0.203s, episode steps:  30, steps per second: 148, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.433 [0.000, 1.000],  loss: 0.053980, mae: 1.495512, mean_q: 2.903504, mean_eps: 0.752005\n",
            " 2813/10000: episode: 96, duration: 0.289s, episode steps:  42, steps per second: 146, episode reward: 42.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  loss: 0.047318, mae: 1.492065, mean_q: 2.896154, mean_eps: 0.748765\n",
            " 2844/10000: episode: 97, duration: 0.194s, episode steps:  31, steps per second: 160, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.516 [0.000, 1.000],  loss: 0.044983, mae: 1.490906, mean_q: 2.898003, mean_eps: 0.745480\n",
            " 2860/10000: episode: 98, duration: 0.106s, episode steps:  16, steps per second: 151, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.037286, mae: 1.487148, mean_q: 2.896274, mean_eps: 0.743365\n",
            " 2895/10000: episode: 99, duration: 0.226s, episode steps:  35, steps per second: 155, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  loss: 0.039602, mae: 1.492188, mean_q: 2.904469, mean_eps: 0.741070\n",
            " 2919/10000: episode: 100, duration: 0.157s, episode steps:  24, steps per second: 153, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.458 [0.000, 1.000],  loss: 0.034912, mae: 1.488765, mean_q: 2.908413, mean_eps: 0.738415\n",
            " 2956/10000: episode: 101, duration: 0.261s, episode steps:  37, steps per second: 142, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.514 [0.000, 1.000],  loss: 0.035224, mae: 1.492676, mean_q: 2.923601, mean_eps: 0.735670\n",
            " 2970/10000: episode: 102, duration: 0.094s, episode steps:  14, steps per second: 150, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.714 [0.000, 1.000],  loss: 0.043769, mae: 1.488700, mean_q: 2.890097, mean_eps: 0.733375\n",
            " 3024/10000: episode: 103, duration: 0.337s, episode steps:  54, steps per second: 160, episode reward: 54.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.463 [0.000, 1.000],  loss: 0.142706, mae: 1.679809, mean_q: 3.043699, mean_eps: 0.730315\n",
            " 3049/10000: episode: 104, duration: 0.160s, episode steps:  25, steps per second: 157, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.440 [0.000, 1.000],  loss: 0.108197, mae: 1.981460, mean_q: 3.855332, mean_eps: 0.726760\n",
            " 3074/10000: episode: 105, duration: 0.171s, episode steps:  25, steps per second: 146, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  loss: 0.093361, mae: 1.957698, mean_q: 3.806621, mean_eps: 0.724510\n",
            " 3084/10000: episode: 106, duration: 0.083s, episode steps:  10, steps per second: 121, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  loss: 0.090263, mae: 1.954380, mean_q: 3.806529, mean_eps: 0.722935\n",
            " 3141/10000: episode: 107, duration: 0.371s, episode steps:  57, steps per second: 154, episode reward: 57.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.509 [0.000, 1.000],  loss: 0.073484, mae: 1.953014, mean_q: 3.819465, mean_eps: 0.719920\n",
            " 3158/10000: episode: 108, duration: 0.110s, episode steps:  17, steps per second: 154, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.588 [0.000, 1.000],  loss: 0.098563, mae: 1.959613, mean_q: 3.812874, mean_eps: 0.716590\n",
            " 3172/10000: episode: 109, duration: 0.089s, episode steps:  14, steps per second: 157, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.357 [0.000, 1.000],  loss: 0.077718, mae: 1.945410, mean_q: 3.773145, mean_eps: 0.715195\n",
            " 3213/10000: episode: 110, duration: 0.257s, episode steps:  41, steps per second: 160, episode reward: 41.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.488 [0.000, 1.000],  loss: 0.081873, mae: 1.955091, mean_q: 3.815518, mean_eps: 0.712720\n",
            " 3251/10000: episode: 111, duration: 0.256s, episode steps:  38, steps per second: 148, episode reward: 38.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  loss: 0.094765, mae: 1.957158, mean_q: 3.806566, mean_eps: 0.709165\n",
            " 3288/10000: episode: 112, duration: 0.263s, episode steps:  37, steps per second: 141, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.514 [0.000, 1.000],  loss: 0.093493, mae: 1.956020, mean_q: 3.796828, mean_eps: 0.705790\n",
            " 3350/10000: episode: 113, duration: 0.381s, episode steps:  62, steps per second: 163, episode reward: 62.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.452 [0.000, 1.000],  loss: 0.089479, mae: 1.958427, mean_q: 3.814876, mean_eps: 0.701335\n",
            " 3369/10000: episode: 114, duration: 0.134s, episode steps:  19, steps per second: 142, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  loss: 0.106299, mae: 1.955052, mean_q: 3.800789, mean_eps: 0.697690\n",
            " 3430/10000: episode: 115, duration: 0.415s, episode steps:  61, steps per second: 147, episode reward: 61.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.459 [0.000, 1.000],  loss: 0.090499, mae: 1.957989, mean_q: 3.800329, mean_eps: 0.694090\n",
            " 3464/10000: episode: 116, duration: 0.315s, episode steps:  34, steps per second: 108, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.068587, mae: 1.963677, mean_q: 3.837149, mean_eps: 0.689815\n",
            " 3527/10000: episode: 117, duration: 0.581s, episode steps:  63, steps per second: 108, episode reward: 63.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.508 [0.000, 1.000],  loss: 0.076308, mae: 1.961131, mean_q: 3.828931, mean_eps: 0.685450\n",
            " 3582/10000: episode: 118, duration: 0.479s, episode steps:  55, steps per second: 115, episode reward: 55.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.527 [0.000, 1.000],  loss: 0.084226, mae: 1.956124, mean_q: 3.799867, mean_eps: 0.680140\n",
            " 3600/10000: episode: 119, duration: 0.171s, episode steps:  18, steps per second: 105, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.556 [0.000, 1.000],  loss: 0.097500, mae: 1.972246, mean_q: 3.831916, mean_eps: 0.676855\n",
            " 3647/10000: episode: 120, duration: 0.448s, episode steps:  47, steps per second: 105, episode reward: 47.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.489 [0.000, 1.000],  loss: 0.086552, mae: 1.956647, mean_q: 3.800563, mean_eps: 0.673930\n",
            " 3700/10000: episode: 121, duration: 0.501s, episode steps:  53, steps per second: 106, episode reward: 53.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.491 [0.000, 1.000],  loss: 0.070418, mae: 1.961223, mean_q: 3.833491, mean_eps: 0.669430\n",
            " 3780/10000: episode: 122, duration: 0.536s, episode steps:  80, steps per second: 149, episode reward: 80.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.463 [0.000, 1.000],  loss: 0.066073, mae: 1.960636, mean_q: 3.833588, mean_eps: 0.663445\n",
            " 3812/10000: episode: 123, duration: 0.199s, episode steps:  32, steps per second: 161, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.081696, mae: 1.959631, mean_q: 3.817220, mean_eps: 0.658405\n",
            " 3822/10000: episode: 124, duration: 0.068s, episode steps:  10, steps per second: 147, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.700 [0.000, 1.000],  loss: 0.097165, mae: 1.955109, mean_q: 3.788820, mean_eps: 0.656515\n",
            " 3835/10000: episode: 125, duration: 0.085s, episode steps:  13, steps per second: 152, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.615 [0.000, 1.000],  loss: 0.066348, mae: 1.941660, mean_q: 3.799960, mean_eps: 0.655480\n",
            " 3880/10000: episode: 126, duration: 0.295s, episode steps:  45, steps per second: 153, episode reward: 45.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.378 [0.000, 1.000],  loss: 0.083513, mae: 1.959379, mean_q: 3.832792, mean_eps: 0.652870\n",
            " 3931/10000: episode: 127, duration: 0.332s, episode steps:  51, steps per second: 154, episode reward: 51.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 0.086498, mae: 1.954904, mean_q: 3.808840, mean_eps: 0.648550\n",
            " 4008/10000: episode: 128, duration: 0.486s, episode steps:  77, steps per second: 158, episode reward: 77.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  loss: 0.112141, mae: 1.996406, mean_q: 3.826437, mean_eps: 0.642790\n",
            " 4065/10000: episode: 129, duration: 0.361s, episode steps:  57, steps per second: 158, episode reward: 57.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 0.174728, mae: 2.414082, mean_q: 4.631151, mean_eps: 0.636760\n",
            " 4140/10000: episode: 130, duration: 0.469s, episode steps:  75, steps per second: 160, episode reward: 75.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  loss: 0.100362, mae: 2.391596, mean_q: 4.710816, mean_eps: 0.630820\n",
            " 4195/10000: episode: 131, duration: 0.353s, episode steps:  55, steps per second: 156, episode reward: 55.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.509 [0.000, 1.000],  loss: 0.106449, mae: 2.397531, mean_q: 4.723673, mean_eps: 0.624970\n",
            " 4215/10000: episode: 132, duration: 0.129s, episode steps:  20, steps per second: 155, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.450 [0.000, 1.000],  loss: 0.098114, mae: 2.394865, mean_q: 4.706819, mean_eps: 0.621595\n",
            " 4328/10000: episode: 133, duration: 0.710s, episode steps: 113, steps per second: 159, episode reward: 113.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.478 [0.000, 1.000],  loss: 0.103792, mae: 2.398672, mean_q: 4.710802, mean_eps: 0.615610\n",
            " 4366/10000: episode: 134, duration: 0.249s, episode steps:  38, steps per second: 153, episode reward: 38.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.447 [0.000, 1.000],  loss: 0.116794, mae: 2.412381, mean_q: 4.736440, mean_eps: 0.608815\n",
            " 4435/10000: episode: 135, duration: 0.435s, episode steps:  69, steps per second: 159, episode reward: 69.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.478 [0.000, 1.000],  loss: 0.110544, mae: 2.404442, mean_q: 4.706675, mean_eps: 0.604000\n",
            " 4525/10000: episode: 136, duration: 0.567s, episode steps:  90, steps per second: 159, episode reward: 90.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.511 [0.000, 1.000],  loss: 0.130620, mae: 2.410100, mean_q: 4.712744, mean_eps: 0.596845\n",
            " 4576/10000: episode: 137, duration: 0.324s, episode steps:  51, steps per second: 158, episode reward: 51.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.569 [0.000, 1.000],  loss: 0.100103, mae: 2.410659, mean_q: 4.725997, mean_eps: 0.590500\n",
            " 4614/10000: episode: 138, duration: 0.248s, episode steps:  38, steps per second: 153, episode reward: 38.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.105396, mae: 2.402640, mean_q: 4.702484, mean_eps: 0.586495\n",
            " 4629/10000: episode: 139, duration: 0.100s, episode steps:  15, steps per second: 150, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  loss: 0.092248, mae: 2.407224, mean_q: 4.717674, mean_eps: 0.584110\n",
            " 4654/10000: episode: 140, duration: 0.164s, episode steps:  25, steps per second: 152, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  loss: 0.076947, mae: 2.405731, mean_q: 4.738425, mean_eps: 0.582310\n",
            " 4762/10000: episode: 141, duration: 0.657s, episode steps: 108, steps per second: 164, episode reward: 108.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  loss: 0.099227, mae: 2.410966, mean_q: 4.730683, mean_eps: 0.576325\n",
            " 4816/10000: episode: 142, duration: 0.355s, episode steps:  54, steps per second: 152, episode reward: 54.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.481 [0.000, 1.000],  loss: 0.105756, mae: 2.410480, mean_q: 4.719918, mean_eps: 0.569035\n",
            " 4886/10000: episode: 143, duration: 0.473s, episode steps:  70, steps per second: 148, episode reward: 70.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.090050, mae: 2.416325, mean_q: 4.739489, mean_eps: 0.563455\n",
            " 4996/10000: episode: 144, duration: 0.692s, episode steps: 110, steps per second: 159, episode reward: 110.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.464 [0.000, 1.000],  loss: 0.105262, mae: 2.410297, mean_q: 4.723247, mean_eps: 0.555355\n",
            " 5012/10000: episode: 145, duration: 0.102s, episode steps:  16, steps per second: 156, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.438 [0.000, 1.000],  loss: 0.308705, mae: 2.713002, mean_q: 4.884940, mean_eps: 0.549685\n",
            " 5164/10000: episode: 146, duration: 0.949s, episode steps: 152, steps per second: 160, episode reward: 152.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  loss: 0.155645, mae: 2.872664, mean_q: 5.634596, mean_eps: 0.542125\n",
            " 5255/10000: episode: 147, duration: 0.745s, episode steps:  91, steps per second: 122, episode reward: 91.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.505 [0.000, 1.000],  loss: 0.155335, mae: 2.874327, mean_q: 5.627940, mean_eps: 0.531190\n",
            " 5290/10000: episode: 148, duration: 0.365s, episode steps:  35, steps per second:  96, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.514 [0.000, 1.000],  loss: 0.173308, mae: 2.866403, mean_q: 5.607243, mean_eps: 0.525520\n",
            " 5361/10000: episode: 149, duration: 0.639s, episode steps:  71, steps per second: 111, episode reward: 71.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.521 [0.000, 1.000],  loss: 0.101124, mae: 2.882260, mean_q: 5.680707, mean_eps: 0.520750\n",
            " 5382/10000: episode: 150, duration: 0.225s, episode steps:  21, steps per second:  93, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  loss: 0.113683, mae: 2.888852, mean_q: 5.704062, mean_eps: 0.516610\n",
            " 5514/10000: episode: 151, duration: 1.235s, episode steps: 132, steps per second: 107, episode reward: 132.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.530 [0.000, 1.000],  loss: 0.143597, mae: 2.880016, mean_q: 5.668485, mean_eps: 0.509725\n",
            " 5645/10000: episode: 152, duration: 1.324s, episode steps: 131, steps per second:  99, episode reward: 131.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.511 [0.000, 1.000],  loss: 0.134008, mae: 2.871876, mean_q: 5.661755, mean_eps: 0.497890\n",
            " 5831/10000: episode: 153, duration: 1.463s, episode steps: 186, steps per second: 127, episode reward: 186.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: 0.115168, mae: 2.876500, mean_q: 5.685086, mean_eps: 0.483625\n",
            " 5985/10000: episode: 154, duration: 0.966s, episode steps: 154, steps per second: 159, episode reward: 154.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 0.129117, mae: 2.877677, mean_q: 5.670077, mean_eps: 0.468325\n",
            " 6083/10000: episode: 155, duration: 0.600s, episode steps:  98, steps per second: 163, episode reward: 98.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  loss: 0.194002, mae: 3.247206, mean_q: 6.311526, mean_eps: 0.456985\n",
            " 6218/10000: episode: 156, duration: 0.826s, episode steps: 135, steps per second: 163, episode reward: 135.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.504 [0.000, 1.000],  loss: 0.158246, mae: 3.295347, mean_q: 6.539363, mean_eps: 0.446500\n",
            " 6393/10000: episode: 157, duration: 1.101s, episode steps: 175, steps per second: 159, episode reward: 175.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.469 [0.000, 1.000],  loss: 0.169511, mae: 3.301488, mean_q: 6.540375, mean_eps: 0.432550\n",
            " 6501/10000: episode: 158, duration: 0.671s, episode steps: 108, steps per second: 161, episode reward: 108.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  loss: 0.136544, mae: 3.304908, mean_q: 6.568212, mean_eps: 0.419815\n",
            " 6647/10000: episode: 159, duration: 0.897s, episode steps: 146, steps per second: 163, episode reward: 146.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.507 [0.000, 1.000],  loss: 0.137227, mae: 3.305148, mean_q: 6.582202, mean_eps: 0.408385\n",
            " 6847/10000: episode: 160, duration: 1.233s, episode steps: 200, steps per second: 162, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.510 [0.000, 1.000],  loss: 0.148544, mae: 3.306323, mean_q: 6.577464, mean_eps: 0.392815\n",
            " 6906/10000: episode: 161, duration: 0.400s, episode steps:  59, steps per second: 147, episode reward: 59.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.458 [0.000, 1.000],  loss: 0.106460, mae: 3.320174, mean_q: 6.635770, mean_eps: 0.381160\n",
            " 7106/10000: episode: 162, duration: 1.286s, episode steps: 200, steps per second: 155, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  loss: 0.192430, mae: 3.556420, mean_q: 7.030480, mean_eps: 0.369505\n",
            " 7306/10000: episode: 163, duration: 1.298s, episode steps: 200, steps per second: 154, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  loss: 0.171136, mae: 3.768811, mean_q: 7.508101, mean_eps: 0.351505\n",
            " 7476/10000: episode: 164, duration: 1.500s, episode steps: 170, steps per second: 113, episode reward: 170.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.541 [0.000, 1.000],  loss: 0.156882, mae: 3.782169, mean_q: 7.537015, mean_eps: 0.334855\n",
            " 7635/10000: episode: 165, duration: 1.241s, episode steps: 159, steps per second: 128, episode reward: 159.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  loss: 0.141207, mae: 3.769404, mean_q: 7.534665, mean_eps: 0.320050\n",
            " 7835/10000: episode: 166, duration: 1.205s, episode steps: 200, steps per second: 166, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  loss: 0.155816, mae: 3.769114, mean_q: 7.532711, mean_eps: 0.303895\n",
            " 8009/10000: episode: 167, duration: 1.054s, episode steps: 174, steps per second: 165, episode reward: 174.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.523 [0.000, 1.000],  loss: 0.180013, mae: 3.799618, mean_q: 7.539168, mean_eps: 0.287065\n",
            " 8159/10000: episode: 168, duration: 0.905s, episode steps: 150, steps per second: 166, episode reward: 150.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.540 [0.000, 1.000],  loss: 0.215939, mae: 4.190680, mean_q: 8.339117, mean_eps: 0.272485\n",
            " 8359/10000: episode: 169, duration: 1.219s, episode steps: 200, steps per second: 164, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.505 [0.000, 1.000],  loss: 0.180269, mae: 4.187967, mean_q: 8.372433, mean_eps: 0.256735\n",
            " 8532/10000: episode: 170, duration: 1.055s, episode steps: 173, steps per second: 164, episode reward: 173.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  loss: 0.191950, mae: 4.199946, mean_q: 8.395121, mean_eps: 0.239950\n",
            " 8732/10000: episode: 171, duration: 1.230s, episode steps: 200, steps per second: 163, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  loss: 0.168799, mae: 4.182967, mean_q: 8.389694, mean_eps: 0.223165\n",
            " 8932/10000: episode: 172, duration: 1.263s, episode steps: 200, steps per second: 158, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.173856, mae: 4.196158, mean_q: 8.420549, mean_eps: 0.205165\n",
            " 9132/10000: episode: 173, duration: 1.247s, episode steps: 200, steps per second: 160, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.505 [0.000, 1.000],  loss: 0.251905, mae: 4.500355, mean_q: 8.943260, mean_eps: 0.187165\n",
            " 9289/10000: episode: 174, duration: 1.290s, episode steps: 157, steps per second: 122, episode reward: 157.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 0.285017, mae: 4.656039, mean_q: 9.290243, mean_eps: 0.171100\n",
            " 9489/10000: episode: 175, duration: 1.751s, episode steps: 200, steps per second: 114, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.510 [0.000, 1.000],  loss: 0.228059, mae: 4.656641, mean_q: 9.322048, mean_eps: 0.155035\n",
            " 9640/10000: episode: 176, duration: 0.941s, episode steps: 151, steps per second: 161, episode reward: 151.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.536 [0.000, 1.000],  loss: 0.233594, mae: 4.654735, mean_q: 9.334450, mean_eps: 0.139240\n",
            " 9840/10000: episode: 177, duration: 1.210s, episode steps: 200, steps per second: 165, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.510 [0.000, 1.000],  loss: 0.212576, mae: 4.653432, mean_q: 9.344141, mean_eps: 0.123445\n",
            "done, took 77.549 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# summarize the history for number  of episode steps\n",
        "plt.plot(history.history['nb_episode_steps'])\n",
        "plt.ylabel('nb_episode_steps')\n",
        "plt.xlabel('episodes')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "bkKOscyvlbDf",
        "outputId": "96464af3-9998-4e07-a1f0-5f7226c459ba"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dqn.test(env, nb_episodes=20, visualize=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2h6WA8hGldSM",
        "outputId": "7a40ef5a-5452-48f7-f847-1050ef7c43a3"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing for 20 episodes ...\n",
            "Episode 1: reward: 200.000, steps: 200\n",
            "Episode 2: reward: 200.000, steps: 200\n",
            "Episode 3: reward: 200.000, steps: 200\n",
            "Episode 4: reward: 200.000, steps: 200\n",
            "Episode 5: reward: 200.000, steps: 200\n",
            "Episode 6: reward: 200.000, steps: 200\n",
            "Episode 7: reward: 200.000, steps: 200\n",
            "Episode 8: reward: 200.000, steps: 200\n",
            "Episode 9: reward: 200.000, steps: 200\n",
            "Episode 10: reward: 200.000, steps: 200\n",
            "Episode 11: reward: 200.000, steps: 200\n",
            "Episode 12: reward: 200.000, steps: 200\n",
            "Episode 13: reward: 200.000, steps: 200\n",
            "Episode 14: reward: 200.000, steps: 200\n",
            "Episode 15: reward: 200.000, steps: 200\n",
            "Episode 16: reward: 200.000, steps: 200\n",
            "Episode 17: reward: 200.000, steps: 200\n",
            "Episode 18: reward: 200.000, steps: 200\n",
            "Episode 19: reward: 200.000, steps: 200\n",
            "Episode 20: reward: 200.000, steps: 200\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f835424aac0>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    }
  ]
}