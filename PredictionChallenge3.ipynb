{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chandhinipj/CartPole-V0-ReinforcementLearning/blob/main/PredictionChallenge3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VKK5DA390wRe"
      },
      "source": [
        "# Skeleton Code for Prediction Challenge 3\n",
        "Below is partial code to get you started on prediction challenge 3. You need to select values for the parameters that have question marks."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install keras-rl2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DNGTghHEhyW4",
        "outputId": "f5aa8824-2264-43a3-aedf-9e9fbe219d55"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting keras-rl2\n",
            "  Downloading keras_rl2-1.0.5-py3-none-any.whl (52 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.1/52.1 KB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tensorflow in /usr/local/lib/python3.9/dist-packages (from keras-rl2) (2.11.0)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (1.22.4)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (3.19.6)\n",
            "Requirement already satisfied: tensorflow-estimator<2.12,>=2.11.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (2.11.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (0.31.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (23.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (3.3.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (3.1.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (0.2.0)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (0.4.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (57.4.0)\n",
            "Requirement already satisfied: tensorboard<2.12,>=2.11 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (2.11.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (1.15.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (1.6.3)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (1.4.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (1.51.3)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (15.0.6.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (4.5.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (2.2.0)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (23.3.3)\n",
            "Requirement already satisfied: keras<2.12,>=2.11.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (2.11.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (1.15.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.9/dist-packages (from astunparse>=1.6.0->tensorflow->keras-rl2) (0.38.4)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow->keras-rl2) (1.8.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow->keras-rl2) (3.4.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow->keras-rl2) (2.25.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow->keras-rl2) (2.2.3)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow->keras-rl2) (2.16.2)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow->keras-rl2) (0.6.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow->keras-rl2) (0.4.6)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow->keras-rl2) (0.2.8)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow->keras-rl2) (5.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow->keras-rl2) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow->keras-rl2) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.9/dist-packages (from markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow->keras-rl2) (6.0.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow->keras-rl2) (1.26.14)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow->keras-rl2) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow->keras-rl2) (2022.12.7)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow->keras-rl2) (4.0.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.9/dist-packages (from werkzeug>=1.0.1->tensorboard<2.12,>=2.11->tensorflow->keras-rl2) (2.1.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow->keras-rl2) (3.15.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.9/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow->keras-rl2) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.9/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow->keras-rl2) (3.2.2)\n",
            "Installing collected packages: keras-rl2\n",
            "Successfully installed keras-rl2-1.0.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gym"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eQ_dXOCKh0jV",
        "outputId": "c74ee187-0e5c-423b-c08f-f9f053ea274c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.9/dist-packages (0.25.2)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.9/dist-packages (from gym) (1.22.4)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.9/dist-packages (from gym) (0.0.8)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.9/dist-packages (from gym) (6.0.0)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.9/dist-packages (from gym) (2.2.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=4.8.0->gym) (3.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load the gym module\n",
        "import gym\n",
        "import matplotlib.pyplot as plt\n",
        "# import the usual Keras modules for creating deep neural networks\n",
        "from keras import Sequential\n",
        "from keras.layers import Input, Flatten, Dense\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "ENV_NAME = 'CartPole-v0'\n",
        "env = gym.make(ENV_NAME)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "75Phf54nh2y_",
        "outputId": "8813905b-18c2-4a1f-e4f7-5bedd3361034"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/gym/envs/registration.py:593: UserWarning: \u001b[33mWARN: The environment CartPole-v0 is out of date. You should consider upgrading to version `v1`.\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.9/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import rl\n",
        "from rl.memory import SequentialMemory  # import the exerience replay buffer module\n",
        "from rl.policy import LinearAnnealedPolicy, EpsGreedyQPolicy  # import the policy\n",
        "from rl.agents.dqn import DQNAgent      # import the DQN agent"
      ],
      "metadata": {
        "id": "KsGYDOrDh58e"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q-Network\n",
        "model = Sequential()\n",
        "model.add(Input(shape=(1,env.observation_space.shape[0])))  # The input is 1 observation vector, and the number of observations in that vector \n",
        "model.add(Flatten())\n",
        "# add extra layers here\n",
        "model.add(Dense(16, activation='relu'))\n",
        "# model.add(Dense(32, activation='relu'))\n",
        "model.add(Dense(env.action_space.n, activation='linear'))   # the output is the number of actions in the action space\n",
        "print(model.summary())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i5dvD8xTio0U",
        "outputId": "fcd9a542-56ec-4870-9b40-b140580e105b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten (Flatten)           (None, 4)                 0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 16)                80        \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 2)                 34        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 114\n",
            "Trainable params: 114\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R8ZiiRbxlH2D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a36ce0f1-b560-4443-ef24-0ea686b03e39"
      },
      "source": [
        "memory = SequentialMemory(limit=10000, window_length=1)\n",
        "\n",
        "# define the policy\n",
        "policy =  LinearAnnealedPolicy(inner_policy=EpsGreedyQPolicy(), \n",
        "                               attr='eps',            \n",
        "                               value_max=1.0,\n",
        "                               value_min=0.1, \n",
        "                               value_test=.05,\n",
        "                               nb_steps=10000)\n",
        "\n",
        "\n",
        "# define the agent\n",
        "dqn = DQNAgent(model=model, \n",
        "               nb_actions=env.action_space.n,\n",
        "               memory=memory,\n",
        "               nb_steps_warmup=20,\n",
        "               target_model_update=1e-2, \n",
        "               policy=policy) \n",
        "\n",
        "dqn.compile(Adam(lr=1e-3), metrics=['mae'])\n",
        "\n",
        "history = dqn.fit(env, nb_steps=10000, visualize=False, verbose=2)\n",
        "\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/optimizers/optimizer_v2/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.9/dist-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
            "Instructions for updating:\n",
            "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n",
            "/usr/local/lib/python3.9/dist-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates=self.state_updates,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training for 10000 steps ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/rl/memory.py:37: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
            "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n",
            "/usr/local/lib/python3.9/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 20 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
            "/usr/local/lib/python3.9/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 21 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
            "/usr/local/lib/python3.9/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 22 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
            "/usr/local/lib/python3.9/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 23 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
            "/usr/local/lib/python3.9/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 24 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
            "/usr/local/lib/python3.9/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 25 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
            "/usr/local/lib/python3.9/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 26 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
            "/usr/local/lib/python3.9/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 27 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
            "/usr/local/lib/python3.9/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 28 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
            "/usr/local/lib/python3.9/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 29 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
            "/usr/local/lib/python3.9/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 30 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
            "/usr/local/lib/python3.9/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 31 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   37/10000: episode: 1, duration: 1.027s, episode steps:  37, steps per second:  36, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.676 [0.000, 1.000],  loss: 0.657054, mae: 0.656707, mean_q: 0.242061, mean_eps: 0.997435\n",
            "   61/10000: episode: 2, duration: 0.234s, episode steps:  24, steps per second: 103, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.458 [0.000, 1.000],  loss: 0.592835, mae: 0.659373, mean_q: 0.452474, mean_eps: 0.995635\n",
            "   98/10000: episode: 3, duration: 0.315s, episode steps:  37, steps per second: 118, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.486 [0.000, 1.000],  loss: 0.449781, mae: 0.591318, mean_q: 0.519009, mean_eps: 0.992890\n",
            "  127/10000: episode: 4, duration: 0.232s, episode steps:  29, steps per second: 125, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.483 [0.000, 1.000],  loss: 0.371907, mae: 0.622583, mean_q: 0.654143, mean_eps: 0.989920\n",
            "  157/10000: episode: 5, duration: 0.230s, episode steps:  30, steps per second: 130, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  loss: 0.319391, mae: 0.675189, mean_q: 0.817135, mean_eps: 0.987265\n",
            "  173/10000: episode: 6, duration: 0.109s, episode steps:  16, steps per second: 147, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.438 [0.000, 1.000],  loss: 0.298468, mae: 0.751597, mean_q: 1.012656, mean_eps: 0.985195\n",
            "  204/10000: episode: 7, duration: 0.237s, episode steps:  31, steps per second: 131, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.484 [0.000, 1.000],  loss: 0.257330, mae: 0.794384, mean_q: 1.156076, mean_eps: 0.983080\n",
            "  223/10000: episode: 8, duration: 0.136s, episode steps:  19, steps per second: 140, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.579 [0.000, 1.000],  loss: 0.211918, mae: 0.869196, mean_q: 1.317602, mean_eps: 0.980830\n",
            "  241/10000: episode: 9, duration: 0.132s, episode steps:  18, steps per second: 137, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 0.210811, mae: 0.942496, mean_q: 1.465827, mean_eps: 0.979165\n",
            "  273/10000: episode: 10, duration: 0.227s, episode steps:  32, steps per second: 141, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.656 [0.000, 1.000],  loss: 0.179790, mae: 1.018635, mean_q: 1.704994, mean_eps: 0.976915\n",
            "  299/10000: episode: 11, duration: 0.192s, episode steps:  26, steps per second: 136, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: 0.193260, mae: 1.145157, mean_q: 1.980237, mean_eps: 0.974305\n",
            "  309/10000: episode: 12, duration: 0.070s, episode steps:  10, steps per second: 144, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.300 [0.000, 1.000],  loss: 0.154180, mae: 1.194798, mean_q: 2.085965, mean_eps: 0.972685\n",
            "  329/10000: episode: 13, duration: 0.130s, episode steps:  20, steps per second: 154, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  loss: 0.211015, mae: 1.281918, mean_q: 2.275414, mean_eps: 0.971335\n",
            "  353/10000: episode: 14, duration: 0.165s, episode steps:  24, steps per second: 145, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.542 [0.000, 1.000],  loss: 0.176740, mae: 1.351629, mean_q: 2.415598, mean_eps: 0.969355\n",
            "  377/10000: episode: 15, duration: 0.217s, episode steps:  24, steps per second: 111, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.542 [0.000, 1.000],  loss: 0.181532, mae: 1.447470, mean_q: 2.622877, mean_eps: 0.967195\n",
            "  388/10000: episode: 16, duration: 0.115s, episode steps:  11, steps per second:  96, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  loss: 0.192532, mae: 1.525369, mean_q: 2.813273, mean_eps: 0.965620\n",
            "  399/10000: episode: 17, duration: 0.207s, episode steps:  11, steps per second:  53, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.273 [0.000, 1.000],  loss: 0.184390, mae: 1.584318, mean_q: 2.964572, mean_eps: 0.964630\n",
            "  453/10000: episode: 18, duration: 1.508s, episode steps:  54, steps per second:  36, episode reward: 54.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.198003, mae: 1.705162, mean_q: 3.222985, mean_eps: 0.961705\n",
            "  464/10000: episode: 19, duration: 0.291s, episode steps:  11, steps per second:  38, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.182 [0.000, 1.000],  loss: 0.222074, mae: 1.819749, mean_q: 3.453820, mean_eps: 0.958780\n",
            "  488/10000: episode: 20, duration: 0.859s, episode steps:  24, steps per second:  28, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.417 [0.000, 1.000],  loss: 0.296372, mae: 1.947298, mean_q: 3.688720, mean_eps: 0.957205\n",
            "  528/10000: episode: 21, duration: 0.934s, episode steps:  40, steps per second:  43, episode reward: 40.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  loss: 0.278704, mae: 2.087272, mean_q: 3.983718, mean_eps: 0.954325\n",
            "  542/10000: episode: 22, duration: 0.221s, episode steps:  14, steps per second:  63, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  loss: 0.221859, mae: 2.216831, mean_q: 4.333735, mean_eps: 0.951895\n",
            "  565/10000: episode: 23, duration: 0.328s, episode steps:  23, steps per second:  70, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.478 [0.000, 1.000],  loss: 0.256732, mae: 2.302363, mean_q: 4.503735, mean_eps: 0.950230\n",
            "  575/10000: episode: 24, duration: 0.181s, episode steps:  10, steps per second:  55, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.800 [0.000, 1.000],  loss: 0.251258, mae: 2.412958, mean_q: 4.745848, mean_eps: 0.948745\n",
            "  598/10000: episode: 25, duration: 0.276s, episode steps:  23, steps per second:  83, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  loss: 0.407700, mae: 2.548647, mean_q: 4.943770, mean_eps: 0.947260\n",
            "  614/10000: episode: 26, duration: 0.228s, episode steps:  16, steps per second:  70, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.438 [0.000, 1.000],  loss: 0.513238, mae: 2.641493, mean_q: 5.074296, mean_eps: 0.945505\n",
            "  626/10000: episode: 27, duration: 0.160s, episode steps:  12, steps per second:  75, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.417 [0.000, 1.000],  loss: 0.705856, mae: 2.784917, mean_q: 5.262593, mean_eps: 0.944245\n",
            "  654/10000: episode: 28, duration: 0.506s, episode steps:  28, steps per second:  55, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.464 [0.000, 1.000],  loss: 0.640676, mae: 2.846141, mean_q: 5.395127, mean_eps: 0.942445\n",
            "  698/10000: episode: 29, duration: 0.576s, episode steps:  44, steps per second:  76, episode reward: 44.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.477 [0.000, 1.000],  loss: 0.643063, mae: 3.023370, mean_q: 5.827774, mean_eps: 0.939205\n",
            "  712/10000: episode: 30, duration: 0.168s, episode steps:  14, steps per second:  83, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.714 [0.000, 1.000],  loss: 0.689665, mae: 3.160243, mean_q: 6.044417, mean_eps: 0.936595\n",
            "  723/10000: episode: 31, duration: 0.164s, episode steps:  11, steps per second:  67, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.273 [0.000, 1.000],  loss: 0.930753, mae: 3.298393, mean_q: 6.290752, mean_eps: 0.935470\n",
            "  745/10000: episode: 32, duration: 0.246s, episode steps:  22, steps per second:  89, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.364 [0.000, 1.000],  loss: 0.607500, mae: 3.226783, mean_q: 6.226134, mean_eps: 0.933985\n",
            "  770/10000: episode: 33, duration: 0.374s, episode steps:  25, steps per second:  67, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  loss: 0.583068, mae: 3.365455, mean_q: 6.616795, mean_eps: 0.931870\n",
            "  812/10000: episode: 34, duration: 0.826s, episode steps:  42, steps per second:  51, episode reward: 42.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  loss: 0.927163, mae: 3.601364, mean_q: 7.047204, mean_eps: 0.928855\n",
            "  822/10000: episode: 35, duration: 0.185s, episode steps:  10, steps per second:  54, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.200 [0.000, 1.000],  loss: 0.488929, mae: 3.623859, mean_q: 7.164305, mean_eps: 0.926515\n",
            "  861/10000: episode: 36, duration: 0.594s, episode steps:  39, steps per second:  66, episode reward: 39.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.436 [0.000, 1.000],  loss: 0.931611, mae: 3.911748, mean_q: 7.743780, mean_eps: 0.924310\n",
            "  893/10000: episode: 37, duration: 0.508s, episode steps:  32, steps per second:  63, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 1.196082, mae: 4.093555, mean_q: 8.031318, mean_eps: 0.921115\n",
            "  930/10000: episode: 38, duration: 0.694s, episode steps:  37, steps per second:  53, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.486 [0.000, 1.000],  loss: 1.549167, mae: 4.286797, mean_q: 8.339491, mean_eps: 0.918010\n",
            "  961/10000: episode: 39, duration: 0.478s, episode steps:  31, steps per second:  65, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.484 [0.000, 1.000],  loss: 1.108453, mae: 4.470855, mean_q: 8.718478, mean_eps: 0.914950\n",
            "  981/10000: episode: 40, duration: 0.265s, episode steps:  20, steps per second:  76, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  loss: 1.670378, mae: 4.648306, mean_q: 9.092888, mean_eps: 0.912655\n",
            "  998/10000: episode: 41, duration: 0.187s, episode steps:  17, steps per second:  91, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.353 [0.000, 1.000],  loss: 1.873047, mae: 4.754202, mean_q: 9.120468, mean_eps: 0.910990\n",
            " 1019/10000: episode: 42, duration: 0.280s, episode steps:  21, steps per second:  75, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 0.846192, mae: 4.713805, mean_q: 9.310493, mean_eps: 0.909280\n",
            " 1036/10000: episode: 43, duration: 0.278s, episode steps:  17, steps per second:  61, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 2.162859, mae: 4.992244, mean_q: 9.709544, mean_eps: 0.907570\n",
            " 1053/10000: episode: 44, duration: 0.250s, episode steps:  17, steps per second:  68, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.706 [0.000, 1.000],  loss: 1.540087, mae: 4.976576, mean_q: 9.605366, mean_eps: 0.906040\n",
            " 1066/10000: episode: 45, duration: 0.162s, episode steps:  13, steps per second:  80, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.308 [0.000, 1.000],  loss: 2.055435, mae: 5.104141, mean_q: 9.858790, mean_eps: 0.904690\n",
            " 1110/10000: episode: 46, duration: 0.458s, episode steps:  44, steps per second:  96, episode reward: 44.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 2.068703, mae: 5.208419, mean_q: 10.049099, mean_eps: 0.902125\n",
            " 1134/10000: episode: 47, duration: 0.333s, episode steps:  24, steps per second:  72, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 1.695314, mae: 5.331563, mean_q: 10.358123, mean_eps: 0.899065\n",
            " 1146/10000: episode: 48, duration: 0.153s, episode steps:  12, steps per second:  79, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.250 [0.000, 1.000],  loss: 2.318979, mae: 5.541557, mean_q: 10.742860, mean_eps: 0.897445\n",
            " 1161/10000: episode: 49, duration: 0.388s, episode steps:  15, steps per second:  39, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  loss: 1.961466, mae: 5.547022, mean_q: 10.718864, mean_eps: 0.896230\n",
            " 1176/10000: episode: 50, duration: 0.542s, episode steps:  15, steps per second:  28, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  loss: 1.966882, mae: 5.565006, mean_q: 10.765720, mean_eps: 0.894880\n",
            " 1188/10000: episode: 51, duration: 0.451s, episode steps:  12, steps per second:  27, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.250 [0.000, 1.000],  loss: 2.337716, mae: 5.701114, mean_q: 11.092413, mean_eps: 0.893665\n",
            " 1203/10000: episode: 52, duration: 0.699s, episode steps:  15, steps per second:  21, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  loss: 2.253379, mae: 5.715869, mean_q: 11.194565, mean_eps: 0.892450\n",
            " 1264/10000: episode: 53, duration: 1.504s, episode steps:  61, steps per second:  41, episode reward: 61.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.410 [0.000, 1.000],  loss: 2.307463, mae: 5.899971, mean_q: 11.340794, mean_eps: 0.889030\n",
            " 1281/10000: episode: 54, duration: 0.228s, episode steps:  17, steps per second:  75, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.294 [0.000, 1.000],  loss: 1.948211, mae: 6.026511, mean_q: 11.734609, mean_eps: 0.885520\n",
            " 1295/10000: episode: 55, duration: 0.210s, episode steps:  14, steps per second:  67, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.286 [0.000, 1.000],  loss: 2.029257, mae: 6.126420, mean_q: 11.935547, mean_eps: 0.884125\n",
            " 1311/10000: episode: 56, duration: 0.255s, episode steps:  16, steps per second:  63, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.312 [0.000, 1.000],  loss: 2.760222, mae: 6.267467, mean_q: 12.097750, mean_eps: 0.882775\n",
            " 1343/10000: episode: 57, duration: 0.411s, episode steps:  32, steps per second:  78, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.406 [0.000, 1.000],  loss: 3.094145, mae: 6.409452, mean_q: 12.190443, mean_eps: 0.880615\n",
            " 1355/10000: episode: 58, duration: 0.135s, episode steps:  12, steps per second:  89, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.250 [0.000, 1.000],  loss: 2.099218, mae: 6.342498, mean_q: 12.324943, mean_eps: 0.878635\n",
            " 1369/10000: episode: 59, duration: 0.187s, episode steps:  14, steps per second:  75, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  loss: 2.657395, mae: 6.508866, mean_q: 12.518900, mean_eps: 0.877465\n",
            " 1384/10000: episode: 60, duration: 0.187s, episode steps:  15, steps per second:  80, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  loss: 2.642855, mae: 6.505772, mean_q: 12.581814, mean_eps: 0.876160\n",
            " 1400/10000: episode: 61, duration: 0.236s, episode steps:  16, steps per second:  68, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 3.841456, mae: 6.669038, mean_q: 12.544280, mean_eps: 0.874765\n",
            " 1428/10000: episode: 62, duration: 0.299s, episode steps:  28, steps per second:  94, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 3.038174, mae: 6.685201, mean_q: 12.622097, mean_eps: 0.872785\n",
            " 1438/10000: episode: 63, duration: 0.106s, episode steps:  10, steps per second:  94, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.300 [0.000, 1.000],  loss: 3.764587, mae: 6.762422, mean_q: 12.866333, mean_eps: 0.871075\n",
            " 1466/10000: episode: 64, duration: 0.350s, episode steps:  28, steps per second:  80, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.357 [0.000, 1.000],  loss: 2.708933, mae: 6.749330, mean_q: 12.946558, mean_eps: 0.869365\n",
            " 1485/10000: episode: 65, duration: 0.269s, episode steps:  19, steps per second:  71, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.421 [0.000, 1.000],  loss: 2.728157, mae: 6.820074, mean_q: 13.144166, mean_eps: 0.867250\n",
            " 1501/10000: episode: 66, duration: 0.298s, episode steps:  16, steps per second:  54, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.688 [0.000, 1.000],  loss: 3.267495, mae: 6.946811, mean_q: 13.180814, mean_eps: 0.865675\n",
            " 1515/10000: episode: 67, duration: 0.267s, episode steps:  14, steps per second:  53, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.143 [0.000, 1.000],  loss: 2.898121, mae: 6.933484, mean_q: 13.114442, mean_eps: 0.864325\n",
            " 1530/10000: episode: 68, duration: 0.125s, episode steps:  15, steps per second: 120, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.733 [0.000, 1.000],  loss: 3.026996, mae: 7.034908, mean_q: 13.298761, mean_eps: 0.863020\n",
            " 1544/10000: episode: 69, duration: 0.106s, episode steps:  14, steps per second: 132, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.643 [0.000, 1.000],  loss: 2.877114, mae: 7.047937, mean_q: 13.266257, mean_eps: 0.861715\n",
            " 1565/10000: episode: 70, duration: 0.148s, episode steps:  21, steps per second: 141, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.714 [0.000, 1.000],  loss: 2.765758, mae: 7.054199, mean_q: 13.404955, mean_eps: 0.860140\n",
            " 1592/10000: episode: 71, duration: 0.221s, episode steps:  27, steps per second: 122, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.444 [0.000, 1.000],  loss: 2.615164, mae: 7.115293, mean_q: 13.590885, mean_eps: 0.857980\n",
            " 1601/10000: episode: 72, duration: 0.083s, episode steps:   9, steps per second: 108, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.222 [0.000, 1.000],  loss: 3.343595, mae: 7.207678, mean_q: 13.626373, mean_eps: 0.856360\n",
            " 1613/10000: episode: 73, duration: 0.094s, episode steps:  12, steps per second: 127, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.250 [0.000, 1.000],  loss: 2.475479, mae: 7.198993, mean_q: 13.687015, mean_eps: 0.855415\n",
            " 1633/10000: episode: 74, duration: 0.179s, episode steps:  20, steps per second: 111, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.300 [0.000, 1.000],  loss: 3.651075, mae: 7.308425, mean_q: 13.801390, mean_eps: 0.853975\n",
            " 1652/10000: episode: 75, duration: 0.153s, episode steps:  19, steps per second: 124, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  loss: 2.496832, mae: 7.239839, mean_q: 13.770439, mean_eps: 0.852220\n",
            " 1667/10000: episode: 76, duration: 0.117s, episode steps:  15, steps per second: 128, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 3.844867, mae: 7.394794, mean_q: 13.824117, mean_eps: 0.850690\n",
            " 1697/10000: episode: 77, duration: 0.222s, episode steps:  30, steps per second: 135, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 3.886168, mae: 7.429594, mean_q: 13.747891, mean_eps: 0.848665\n",
            " 1730/10000: episode: 78, duration: 0.252s, episode steps:  33, steps per second: 131, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.455 [0.000, 1.000],  loss: 3.481605, mae: 7.454946, mean_q: 13.865874, mean_eps: 0.845830\n",
            " 1765/10000: episode: 79, duration: 0.264s, episode steps:  35, steps per second: 133, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.457 [0.000, 1.000],  loss: 3.055157, mae: 7.483138, mean_q: 14.103896, mean_eps: 0.842770\n",
            " 1821/10000: episode: 80, duration: 0.440s, episode steps:  56, steps per second: 127, episode reward: 56.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.482 [0.000, 1.000],  loss: 3.764479, mae: 7.572412, mean_q: 14.047824, mean_eps: 0.838675\n",
            " 1876/10000: episode: 81, duration: 0.446s, episode steps:  55, steps per second: 123, episode reward: 55.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.509 [0.000, 1.000],  loss: 3.210715, mae: 7.620137, mean_q: 14.329160, mean_eps: 0.833680\n",
            " 1897/10000: episode: 82, duration: 0.149s, episode steps:  21, steps per second: 141, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  loss: 3.373763, mae: 7.715795, mean_q: 14.512282, mean_eps: 0.830260\n",
            " 1916/10000: episode: 83, duration: 0.153s, episode steps:  19, steps per second: 124, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 3.865839, mae: 7.724232, mean_q: 14.417792, mean_eps: 0.828460\n",
            " 1958/10000: episode: 84, duration: 0.279s, episode steps:  42, steps per second: 151, episode reward: 42.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.452 [0.000, 1.000],  loss: 3.568584, mae: 7.774122, mean_q: 14.562600, mean_eps: 0.825715\n",
            " 1970/10000: episode: 85, duration: 0.077s, episode steps:  12, steps per second: 156, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 3.510088, mae: 7.767888, mean_q: 14.595817, mean_eps: 0.823285\n",
            " 2014/10000: episode: 86, duration: 0.293s, episode steps:  44, steps per second: 150, episode reward: 44.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.523 [0.000, 1.000],  loss: 3.272095, mae: 7.807354, mean_q: 14.715610, mean_eps: 0.820765\n",
            " 2083/10000: episode: 87, duration: 0.431s, episode steps:  69, steps per second: 160, episode reward: 69.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.507 [0.000, 1.000],  loss: 3.245747, mae: 7.919131, mean_q: 14.957597, mean_eps: 0.815680\n",
            " 2100/10000: episode: 88, duration: 0.118s, episode steps:  17, steps per second: 144, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 3.057809, mae: 7.994518, mean_q: 15.262387, mean_eps: 0.811810\n",
            " 2113/10000: episode: 89, duration: 0.092s, episode steps:  13, steps per second: 142, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  loss: 3.157801, mae: 8.068948, mean_q: 15.398480, mean_eps: 0.810460\n",
            " 2129/10000: episode: 90, duration: 0.104s, episode steps:  16, steps per second: 154, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.438 [0.000, 1.000],  loss: 3.042697, mae: 8.019205, mean_q: 15.330290, mean_eps: 0.809155\n",
            " 2148/10000: episode: 91, duration: 0.137s, episode steps:  19, steps per second: 138, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.421 [0.000, 1.000],  loss: 2.616997, mae: 8.069024, mean_q: 15.477581, mean_eps: 0.807580\n",
            " 2174/10000: episode: 92, duration: 0.164s, episode steps:  26, steps per second: 158, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 3.774167, mae: 8.207334, mean_q: 15.587629, mean_eps: 0.805555\n",
            " 2200/10000: episode: 93, duration: 0.171s, episode steps:  26, steps per second: 152, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 3.547305, mae: 8.229078, mean_q: 15.679577, mean_eps: 0.803215\n",
            " 2219/10000: episode: 94, duration: 0.124s, episode steps:  19, steps per second: 153, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.632 [0.000, 1.000],  loss: 4.329961, mae: 8.307121, mean_q: 15.666490, mean_eps: 0.801190\n",
            " 2289/10000: episode: 95, duration: 0.428s, episode steps:  70, steps per second: 164, episode reward: 70.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  loss: 3.219767, mae: 8.324750, mean_q: 15.852067, mean_eps: 0.797185\n",
            " 2299/10000: episode: 96, duration: 0.072s, episode steps:  10, steps per second: 140, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  loss: 3.404978, mae: 8.397185, mean_q: 15.958216, mean_eps: 0.793585\n",
            " 2329/10000: episode: 97, duration: 0.205s, episode steps:  30, steps per second: 147, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.433 [0.000, 1.000],  loss: 4.022142, mae: 8.466514, mean_q: 15.970230, mean_eps: 0.791785\n",
            " 2387/10000: episode: 98, duration: 0.408s, episode steps:  58, steps per second: 142, episode reward: 58.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.534 [0.000, 1.000],  loss: 3.606836, mae: 8.524668, mean_q: 16.271144, mean_eps: 0.787825\n",
            " 2421/10000: episode: 99, duration: 0.337s, episode steps:  34, steps per second: 101, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 4.222366, mae: 8.657952, mean_q: 16.380957, mean_eps: 0.783685\n",
            " 2462/10000: episode: 100, duration: 0.377s, episode steps:  41, steps per second: 109, episode reward: 41.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.512 [0.000, 1.000],  loss: 3.904503, mae: 8.731763, mean_q: 16.610414, mean_eps: 0.780310\n",
            " 2486/10000: episode: 101, duration: 0.234s, episode steps:  24, steps per second: 102, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.625 [0.000, 1.000],  loss: 4.261286, mae: 8.793331, mean_q: 16.777251, mean_eps: 0.777385\n",
            " 2521/10000: episode: 102, duration: 0.385s, episode steps:  35, steps per second:  91, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.486 [0.000, 1.000],  loss: 3.923964, mae: 8.861458, mean_q: 16.902188, mean_eps: 0.774730\n",
            " 2542/10000: episode: 103, duration: 0.225s, episode steps:  21, steps per second:  93, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.619 [0.000, 1.000],  loss: 5.015467, mae: 8.954413, mean_q: 17.062792, mean_eps: 0.772210\n",
            " 2573/10000: episode: 104, duration: 0.322s, episode steps:  31, steps per second:  96, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.452 [0.000, 1.000],  loss: 3.734321, mae: 8.942798, mean_q: 17.140995, mean_eps: 0.769870\n",
            " 2609/10000: episode: 105, duration: 0.392s, episode steps:  36, steps per second:  92, episode reward: 36.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.528 [0.000, 1.000],  loss: 3.669749, mae: 9.004479, mean_q: 17.282731, mean_eps: 0.766855\n",
            " 2651/10000: episode: 106, duration: 0.350s, episode steps:  42, steps per second: 120, episode reward: 42.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 3.901699, mae: 9.082454, mean_q: 17.330927, mean_eps: 0.763345\n",
            " 2672/10000: episode: 107, duration: 0.134s, episode steps:  21, steps per second: 156, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  loss: 3.168138, mae: 9.150888, mean_q: 17.779839, mean_eps: 0.760510\n",
            " 2695/10000: episode: 108, duration: 0.154s, episode steps:  23, steps per second: 150, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  loss: 5.606254, mae: 9.302898, mean_q: 17.720433, mean_eps: 0.758530\n",
            " 2734/10000: episode: 109, duration: 0.280s, episode steps:  39, steps per second: 139, episode reward: 39.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: 5.971469, mae: 9.367104, mean_q: 17.662855, mean_eps: 0.755740\n",
            " 2773/10000: episode: 110, duration: 0.286s, episode steps:  39, steps per second: 136, episode reward: 39.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.564 [0.000, 1.000],  loss: 3.892980, mae: 9.374856, mean_q: 17.996407, mean_eps: 0.752230\n",
            " 2799/10000: episode: 111, duration: 0.196s, episode steps:  26, steps per second: 133, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.615 [0.000, 1.000],  loss: 5.264114, mae: 9.492510, mean_q: 18.041544, mean_eps: 0.749305\n",
            " 2815/10000: episode: 112, duration: 0.128s, episode steps:  16, steps per second: 125, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.562 [0.000, 1.000],  loss: 5.853594, mae: 9.515509, mean_q: 17.962451, mean_eps: 0.747415\n",
            " 2829/10000: episode: 113, duration: 0.102s, episode steps:  14, steps per second: 137, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  loss: 3.743324, mae: 9.468790, mean_q: 18.091725, mean_eps: 0.746065\n",
            " 2874/10000: episode: 114, duration: 0.348s, episode steps:  45, steps per second: 129, episode reward: 45.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.489 [0.000, 1.000],  loss: 4.679324, mae: 9.544131, mean_q: 18.255751, mean_eps: 0.743410\n",
            " 2892/10000: episode: 115, duration: 0.131s, episode steps:  18, steps per second: 137, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.611 [0.000, 1.000],  loss: 3.591392, mae: 9.519790, mean_q: 18.381018, mean_eps: 0.740575\n",
            " 2911/10000: episode: 116, duration: 0.157s, episode steps:  19, steps per second: 121, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.632 [0.000, 1.000],  loss: 3.408083, mae: 9.593276, mean_q: 18.556538, mean_eps: 0.738910\n",
            " 2946/10000: episode: 117, duration: 0.250s, episode steps:  35, steps per second: 140, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.543 [0.000, 1.000],  loss: 4.320433, mae: 9.695637, mean_q: 18.597306, mean_eps: 0.736480\n",
            " 2981/10000: episode: 118, duration: 0.251s, episode steps:  35, steps per second: 139, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.486 [0.000, 1.000],  loss: 4.746869, mae: 9.845731, mean_q: 18.864991, mean_eps: 0.733330\n",
            " 3042/10000: episode: 119, duration: 0.489s, episode steps:  61, steps per second: 125, episode reward: 61.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.492 [0.000, 1.000],  loss: 5.523468, mae: 9.925989, mean_q: 18.894323, mean_eps: 0.729010\n",
            " 3073/10000: episode: 120, duration: 0.241s, episode steps:  31, steps per second: 128, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.516 [0.000, 1.000],  loss: 4.888758, mae: 9.979206, mean_q: 19.104483, mean_eps: 0.724870\n",
            " 3096/10000: episode: 121, duration: 0.184s, episode steps:  23, steps per second: 125, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.478 [0.000, 1.000],  loss: 3.090303, mae: 9.928366, mean_q: 19.311857, mean_eps: 0.722440\n",
            " 3124/10000: episode: 122, duration: 0.273s, episode steps:  28, steps per second: 103, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.607 [0.000, 1.000],  loss: 4.926877, mae: 10.043114, mean_q: 19.394331, mean_eps: 0.720145\n",
            " 3167/10000: episode: 123, duration: 0.427s, episode steps:  43, steps per second: 101, episode reward: 43.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.512 [0.000, 1.000],  loss: 4.667840, mae: 10.149155, mean_q: 19.520619, mean_eps: 0.716950\n",
            " 3202/10000: episode: 124, duration: 0.287s, episode steps:  35, steps per second: 122, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.486 [0.000, 1.000],  loss: 5.923279, mae: 10.303963, mean_q: 19.722786, mean_eps: 0.713440\n",
            " 3226/10000: episode: 125, duration: 0.212s, episode steps:  24, steps per second: 113, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 4.902451, mae: 10.317045, mean_q: 19.809057, mean_eps: 0.710785\n",
            " 3235/10000: episode: 126, duration: 0.087s, episode steps:   9, steps per second: 104, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 8.657459, mae: 10.495184, mean_q: 19.781162, mean_eps: 0.709300\n",
            " 3259/10000: episode: 127, duration: 0.199s, episode steps:  24, steps per second: 120, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 5.572349, mae: 10.309486, mean_q: 19.682364, mean_eps: 0.707815\n",
            " 3278/10000: episode: 128, duration: 0.171s, episode steps:  19, steps per second: 111, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.421 [0.000, 1.000],  loss: 6.089653, mae: 10.437191, mean_q: 19.831578, mean_eps: 0.705880\n",
            " 3290/10000: episode: 129, duration: 0.117s, episode steps:  12, steps per second: 102, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.750 [0.000, 1.000],  loss: 7.241708, mae: 10.495803, mean_q: 19.824776, mean_eps: 0.704485\n",
            " 3325/10000: episode: 130, duration: 0.259s, episode steps:  35, steps per second: 135, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.486 [0.000, 1.000],  loss: 5.348612, mae: 10.398221, mean_q: 19.913163, mean_eps: 0.702370\n",
            " 3400/10000: episode: 131, duration: 0.561s, episode steps:  75, steps per second: 134, episode reward: 75.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.453 [0.000, 1.000],  loss: 5.132200, mae: 10.539418, mean_q: 20.239868, mean_eps: 0.697420\n",
            " 3417/10000: episode: 132, duration: 0.124s, episode steps:  17, steps per second: 137, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  loss: 6.021502, mae: 10.638884, mean_q: 20.394193, mean_eps: 0.693280\n",
            " 3451/10000: episode: 133, duration: 0.246s, episode steps:  34, steps per second: 138, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 4.529989, mae: 10.587469, mean_q: 20.481699, mean_eps: 0.690985\n",
            " 3481/10000: episode: 134, duration: 0.232s, episode steps:  30, steps per second: 129, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.567 [0.000, 1.000],  loss: 6.947624, mae: 10.787699, mean_q: 20.506544, mean_eps: 0.688105\n",
            " 3495/10000: episode: 135, duration: 0.103s, episode steps:  14, steps per second: 137, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  loss: 8.015302, mae: 10.814423, mean_q: 20.337146, mean_eps: 0.686125\n",
            " 3541/10000: episode: 136, duration: 0.326s, episode steps:  46, steps per second: 141, episode reward: 46.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.478 [0.000, 1.000],  loss: 5.619018, mae: 10.720696, mean_q: 20.495669, mean_eps: 0.683425\n",
            " 3566/10000: episode: 137, duration: 0.189s, episode steps:  25, steps per second: 132, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.360 [0.000, 1.000],  loss: 7.014184, mae: 10.921998, mean_q: 20.771959, mean_eps: 0.680230\n",
            " 3585/10000: episode: 138, duration: 0.139s, episode steps:  19, steps per second: 136, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.421 [0.000, 1.000],  loss: 5.769552, mae: 10.871688, mean_q: 20.836788, mean_eps: 0.678250\n",
            " 3613/10000: episode: 139, duration: 0.190s, episode steps:  28, steps per second: 147, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 5.422918, mae: 10.842583, mean_q: 20.909439, mean_eps: 0.676135\n",
            " 3685/10000: episode: 140, duration: 0.517s, episode steps:  72, steps per second: 139, episode reward: 72.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.486 [0.000, 1.000],  loss: 6.228073, mae: 11.041336, mean_q: 21.145226, mean_eps: 0.671635\n",
            " 3698/10000: episode: 141, duration: 0.101s, episode steps:  13, steps per second: 129, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.692 [0.000, 1.000],  loss: 4.368510, mae: 10.970787, mean_q: 21.166678, mean_eps: 0.667810\n",
            " 3740/10000: episode: 142, duration: 0.286s, episode steps:  42, steps per second: 147, episode reward: 42.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 6.012782, mae: 11.081101, mean_q: 21.267863, mean_eps: 0.665335\n",
            " 3783/10000: episode: 143, duration: 0.308s, episode steps:  43, steps per second: 139, episode reward: 43.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.488 [0.000, 1.000],  loss: 5.712953, mae: 11.126355, mean_q: 21.328673, mean_eps: 0.661510\n",
            " 3795/10000: episode: 144, duration: 0.092s, episode steps:  12, steps per second: 130, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 4.556312, mae: 11.066467, mean_q: 21.440218, mean_eps: 0.659035\n",
            " 3876/10000: episode: 145, duration: 0.565s, episode steps:  81, steps per second: 143, episode reward: 81.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.469 [0.000, 1.000],  loss: 5.852654, mae: 11.226975, mean_q: 21.587298, mean_eps: 0.654850\n",
            " 3968/10000: episode: 146, duration: 0.704s, episode steps:  92, steps per second: 131, episode reward: 92.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.543 [0.000, 1.000],  loss: 5.855085, mae: 11.350792, mean_q: 21.865306, mean_eps: 0.647065\n",
            " 3986/10000: episode: 147, duration: 0.186s, episode steps:  18, steps per second:  97, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 6.140026, mae: 11.535304, mean_q: 22.155681, mean_eps: 0.642115\n",
            " 4012/10000: episode: 148, duration: 0.265s, episode steps:  26, steps per second:  98, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  loss: 6.703723, mae: 11.526960, mean_q: 22.114660, mean_eps: 0.640135\n",
            " 4029/10000: episode: 149, duration: 0.166s, episode steps:  17, steps per second: 102, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 5.755875, mae: 11.493966, mean_q: 22.111980, mean_eps: 0.638200\n",
            " 4085/10000: episode: 150, duration: 0.619s, episode steps:  56, steps per second:  90, episode reward: 56.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.482 [0.000, 1.000],  loss: 6.560609, mae: 11.647206, mean_q: 22.392469, mean_eps: 0.634915\n",
            " 4094/10000: episode: 151, duration: 0.111s, episode steps:   9, steps per second:  81, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.778 [0.000, 1.000],  loss: 5.927997, mae: 11.605413, mean_q: 22.302330, mean_eps: 0.631990\n",
            " 4118/10000: episode: 152, duration: 0.267s, episode steps:  24, steps per second:  90, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 5.804735, mae: 11.565305, mean_q: 22.330835, mean_eps: 0.630505\n",
            " 4189/10000: episode: 153, duration: 0.681s, episode steps:  71, steps per second: 104, episode reward: 71.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.507 [0.000, 1.000],  loss: 7.036187, mae: 11.777247, mean_q: 22.547537, mean_eps: 0.626230\n",
            " 4229/10000: episode: 154, duration: 0.276s, episode steps:  40, steps per second: 145, episode reward: 40.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 5.289893, mae: 11.799386, mean_q: 22.900192, mean_eps: 0.621235\n",
            " 4249/10000: episode: 155, duration: 0.145s, episode steps:  20, steps per second: 138, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.650 [0.000, 1.000],  loss: 6.251036, mae: 12.014401, mean_q: 23.176437, mean_eps: 0.618535\n",
            " 4307/10000: episode: 156, duration: 0.372s, episode steps:  58, steps per second: 156, episode reward: 58.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 5.435352, mae: 11.893011, mean_q: 23.101413, mean_eps: 0.615025\n",
            " 4321/10000: episode: 157, duration: 0.093s, episode steps:  14, steps per second: 150, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 7.239339, mae: 12.029171, mean_q: 23.223868, mean_eps: 0.611785\n",
            " 4369/10000: episode: 158, duration: 0.317s, episode steps:  48, steps per second: 152, episode reward: 48.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.458 [0.000, 1.000],  loss: 8.219638, mae: 12.130754, mean_q: 23.175620, mean_eps: 0.608995\n",
            " 4458/10000: episode: 159, duration: 0.645s, episode steps:  89, steps per second: 138, episode reward: 89.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.449 [0.000, 1.000],  loss: 7.654661, mae: 12.122421, mean_q: 23.175779, mean_eps: 0.602830\n",
            " 4491/10000: episode: 160, duration: 0.273s, episode steps:  33, steps per second: 121, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  loss: 5.782988, mae: 12.101175, mean_q: 23.514604, mean_eps: 0.597340\n",
            " 4510/10000: episode: 161, duration: 0.153s, episode steps:  19, steps per second: 124, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  loss: 5.078256, mae: 12.163865, mean_q: 23.622026, mean_eps: 0.595000\n",
            " 4562/10000: episode: 162, duration: 0.400s, episode steps:  52, steps per second: 130, episode reward: 52.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  loss: 5.071205, mae: 12.267649, mean_q: 23.934819, mean_eps: 0.591805\n",
            " 4618/10000: episode: 163, duration: 0.409s, episode steps:  56, steps per second: 137, episode reward: 56.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.464 [0.000, 1.000],  loss: 7.331006, mae: 12.457547, mean_q: 23.993552, mean_eps: 0.586945\n",
            " 4712/10000: episode: 164, duration: 0.735s, episode steps:  94, steps per second: 128, episode reward: 94.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 5.741033, mae: 12.511712, mean_q: 24.306454, mean_eps: 0.580195\n",
            " 4752/10000: episode: 165, duration: 0.288s, episode steps:  40, steps per second: 139, episode reward: 40.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.450 [0.000, 1.000],  loss: 7.524275, mae: 12.716642, mean_q: 24.490843, mean_eps: 0.574165\n",
            " 4794/10000: episode: 166, duration: 0.345s, episode steps:  42, steps per second: 122, episode reward: 42.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  loss: 8.849215, mae: 12.756175, mean_q: 24.411486, mean_eps: 0.570475\n",
            " 4841/10000: episode: 167, duration: 0.377s, episode steps:  47, steps per second: 125, episode reward: 47.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.489 [0.000, 1.000],  loss: 8.452210, mae: 12.813083, mean_q: 24.427678, mean_eps: 0.566470\n",
            " 4891/10000: episode: 168, duration: 0.376s, episode steps:  50, steps per second: 133, episode reward: 50.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 5.951718, mae: 12.764039, mean_q: 24.639076, mean_eps: 0.562105\n",
            " 4930/10000: episode: 169, duration: 0.321s, episode steps:  39, steps per second: 122, episode reward: 39.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.436 [0.000, 1.000],  loss: 6.855893, mae: 12.911800, mean_q: 24.970447, mean_eps: 0.558100\n",
            " 4945/10000: episode: 170, duration: 0.115s, episode steps:  15, steps per second: 130, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  loss: 3.262987, mae: 12.732648, mean_q: 25.051535, mean_eps: 0.555670\n",
            " 5002/10000: episode: 171, duration: 0.417s, episode steps:  57, steps per second: 137, episode reward: 57.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 7.325741, mae: 12.989258, mean_q: 25.156899, mean_eps: 0.552430\n",
            " 5101/10000: episode: 172, duration: 0.687s, episode steps:  99, steps per second: 144, episode reward: 99.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.535 [0.000, 1.000],  loss: 6.473036, mae: 13.041142, mean_q: 25.348112, mean_eps: 0.545410\n",
            " 5154/10000: episode: 173, duration: 0.376s, episode steps:  53, steps per second: 141, episode reward: 53.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.472 [0.000, 1.000],  loss: 7.889918, mae: 13.230181, mean_q: 25.574901, mean_eps: 0.538570\n",
            " 5193/10000: episode: 174, duration: 0.296s, episode steps:  39, steps per second: 132, episode reward: 39.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.513 [0.000, 1.000],  loss: 6.822200, mae: 13.266892, mean_q: 25.781337, mean_eps: 0.534430\n",
            " 5242/10000: episode: 175, duration: 0.336s, episode steps:  49, steps per second: 146, episode reward: 49.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 8.940795, mae: 13.301972, mean_q: 25.623789, mean_eps: 0.530470\n",
            " 5278/10000: episode: 176, duration: 0.243s, episode steps:  36, steps per second: 148, episode reward: 36.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.528 [0.000, 1.000],  loss: 10.111828, mae: 13.437343, mean_q: 25.709744, mean_eps: 0.526645\n",
            " 5336/10000: episode: 177, duration: 0.383s, episode steps:  58, steps per second: 151, episode reward: 58.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.466 [0.000, 1.000],  loss: 8.735185, mae: 13.351492, mean_q: 25.768731, mean_eps: 0.522415\n",
            " 5381/10000: episode: 178, duration: 0.323s, episode steps:  45, steps per second: 139, episode reward: 45.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  loss: 5.530997, mae: 13.366970, mean_q: 26.011280, mean_eps: 0.517780\n",
            " 5441/10000: episode: 179, duration: 0.425s, episode steps:  60, steps per second: 141, episode reward: 60.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.567 [0.000, 1.000],  loss: 7.315108, mae: 13.572862, mean_q: 26.302997, mean_eps: 0.513055\n",
            " 5614/10000: episode: 180, duration: 1.388s, episode steps: 173, steps per second: 125, episode reward: 173.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.486 [0.000, 1.000],  loss: 7.623351, mae: 13.638692, mean_q: 26.456772, mean_eps: 0.502570\n",
            " 5651/10000: episode: 181, duration: 0.427s, episode steps:  37, steps per second:  87, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.541 [0.000, 1.000],  loss: 6.644452, mae: 13.811494, mean_q: 26.931402, mean_eps: 0.493120\n",
            " 5727/10000: episode: 182, duration: 0.820s, episode steps:  76, steps per second:  93, episode reward: 76.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 7.846769, mae: 13.906260, mean_q: 27.024265, mean_eps: 0.488035\n",
            " 5800/10000: episode: 183, duration: 0.727s, episode steps:  73, steps per second: 100, episode reward: 73.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.521 [0.000, 1.000],  loss: 7.034127, mae: 14.019304, mean_q: 27.251572, mean_eps: 0.481330\n",
            " 5871/10000: episode: 184, duration: 0.490s, episode steps:  71, steps per second: 145, episode reward: 71.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.465 [0.000, 1.000],  loss: 8.739273, mae: 14.124662, mean_q: 27.291781, mean_eps: 0.474850\n",
            " 5957/10000: episode: 185, duration: 0.566s, episode steps:  86, steps per second: 152, episode reward: 86.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.523 [0.000, 1.000],  loss: 6.664250, mae: 14.151309, mean_q: 27.656178, mean_eps: 0.467785\n",
            " 6046/10000: episode: 186, duration: 0.650s, episode steps:  89, steps per second: 137, episode reward: 89.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.506 [0.000, 1.000],  loss: 8.799166, mae: 14.339681, mean_q: 27.761375, mean_eps: 0.459910\n",
            " 6107/10000: episode: 187, duration: 0.432s, episode steps:  61, steps per second: 141, episode reward: 61.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.541 [0.000, 1.000],  loss: 7.372187, mae: 14.437656, mean_q: 28.125893, mean_eps: 0.453160\n",
            " 6172/10000: episode: 188, duration: 0.481s, episode steps:  65, steps per second: 135, episode reward: 65.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.523 [0.000, 1.000],  loss: 7.959403, mae: 14.523634, mean_q: 28.252735, mean_eps: 0.447490\n",
            " 6226/10000: episode: 189, duration: 0.367s, episode steps:  54, steps per second: 147, episode reward: 54.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.481 [0.000, 1.000],  loss: 7.381083, mae: 14.637263, mean_q: 28.495103, mean_eps: 0.442135\n",
            " 6252/10000: episode: 190, duration: 0.184s, episode steps:  26, steps per second: 141, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  loss: 6.088252, mae: 14.569920, mean_q: 28.505937, mean_eps: 0.438535\n",
            " 6301/10000: episode: 191, duration: 0.326s, episode steps:  49, steps per second: 150, episode reward: 49.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.469 [0.000, 1.000],  loss: 8.995964, mae: 14.770635, mean_q: 28.715374, mean_eps: 0.435160\n",
            " 6348/10000: episode: 192, duration: 0.327s, episode steps:  47, steps per second: 144, episode reward: 47.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.532 [0.000, 1.000],  loss: 8.684809, mae: 14.848465, mean_q: 28.857517, mean_eps: 0.430840\n",
            " 6457/10000: episode: 193, duration: 0.781s, episode steps: 109, steps per second: 140, episode reward: 109.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.505 [0.000, 1.000],  loss: 7.083061, mae: 14.839887, mean_q: 28.974813, mean_eps: 0.423820\n",
            " 6539/10000: episode: 194, duration: 0.595s, episode steps:  82, steps per second: 138, episode reward: 82.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  loss: 7.548850, mae: 15.051747, mean_q: 29.372816, mean_eps: 0.415225\n",
            " 6609/10000: episode: 195, duration: 0.477s, episode steps:  70, steps per second: 147, episode reward: 70.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  loss: 8.503770, mae: 15.175324, mean_q: 29.471326, mean_eps: 0.408385\n",
            " 6683/10000: episode: 196, duration: 0.493s, episode steps:  74, steps per second: 150, episode reward: 74.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.541 [0.000, 1.000],  loss: 8.475494, mae: 15.253512, mean_q: 29.664578, mean_eps: 0.401905\n",
            " 6745/10000: episode: 197, duration: 0.454s, episode steps:  62, steps per second: 136, episode reward: 62.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.468 [0.000, 1.000],  loss: 7.891727, mae: 15.279559, mean_q: 29.730293, mean_eps: 0.395785\n",
            " 6852/10000: episode: 198, duration: 0.784s, episode steps: 107, steps per second: 137, episode reward: 107.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  loss: 7.919049, mae: 15.402079, mean_q: 30.023166, mean_eps: 0.388180\n",
            " 6926/10000: episode: 199, duration: 0.528s, episode steps:  74, steps per second: 140, episode reward: 74.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.459 [0.000, 1.000],  loss: 8.639811, mae: 15.527665, mean_q: 30.234650, mean_eps: 0.380035\n",
            " 7018/10000: episode: 200, duration: 0.710s, episode steps:  92, steps per second: 130, episode reward: 92.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  loss: 7.496389, mae: 15.642253, mean_q: 30.553057, mean_eps: 0.372565\n",
            " 7098/10000: episode: 201, duration: 0.592s, episode steps:  80, steps per second: 135, episode reward: 80.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.537 [0.000, 1.000],  loss: 7.169156, mae: 15.815967, mean_q: 30.959747, mean_eps: 0.364825\n",
            " 7175/10000: episode: 202, duration: 0.569s, episode steps:  77, steps per second: 135, episode reward: 77.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.532 [0.000, 1.000],  loss: 7.921412, mae: 15.848739, mean_q: 30.944825, mean_eps: 0.357760\n",
            " 7233/10000: episode: 203, duration: 0.571s, episode steps:  58, steps per second: 102, episode reward: 58.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.448 [0.000, 1.000],  loss: 7.806926, mae: 15.972350, mean_q: 31.274380, mean_eps: 0.351685\n",
            " 7345/10000: episode: 204, duration: 1.151s, episode steps: 112, steps per second:  97, episode reward: 112.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.509 [0.000, 1.000],  loss: 9.547089, mae: 16.099364, mean_q: 31.372406, mean_eps: 0.344035\n",
            " 7545/10000: episode: 205, duration: 1.701s, episode steps: 200, steps per second: 118, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 9.394018, mae: 16.297173, mean_q: 31.751857, mean_eps: 0.329995\n",
            " 7682/10000: episode: 206, duration: 0.991s, episode steps: 137, steps per second: 138, episode reward: 137.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.518 [0.000, 1.000],  loss: 10.154400, mae: 16.433628, mean_q: 32.024602, mean_eps: 0.314830\n",
            " 7792/10000: episode: 207, duration: 0.851s, episode steps: 110, steps per second: 129, episode reward: 110.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.482 [0.000, 1.000],  loss: 9.543568, mae: 16.555502, mean_q: 32.224881, mean_eps: 0.303715\n",
            " 7849/10000: episode: 208, duration: 0.397s, episode steps:  57, steps per second: 144, episode reward: 57.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  loss: 9.321685, mae: 16.629372, mean_q: 32.435471, mean_eps: 0.296200\n",
            " 7947/10000: episode: 209, duration: 0.684s, episode steps:  98, steps per second: 143, episode reward: 98.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  loss: 6.050747, mae: 16.647653, mean_q: 32.771977, mean_eps: 0.289225\n",
            " 8077/10000: episode: 210, duration: 0.867s, episode steps: 130, steps per second: 150, episode reward: 130.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.477 [0.000, 1.000],  loss: 8.118250, mae: 16.837023, mean_q: 33.083718, mean_eps: 0.278965\n",
            " 8179/10000: episode: 211, duration: 0.685s, episode steps: 102, steps per second: 149, episode reward: 102.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  loss: 9.240805, mae: 17.040216, mean_q: 33.316297, mean_eps: 0.268525\n",
            " 8262/10000: episode: 212, duration: 0.574s, episode steps:  83, steps per second: 145, episode reward: 83.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.470 [0.000, 1.000],  loss: 7.940520, mae: 17.100227, mean_q: 33.603571, mean_eps: 0.260200\n",
            " 8395/10000: episode: 213, duration: 0.949s, episode steps: 133, steps per second: 140, episode reward: 133.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 8.280744, mae: 17.242260, mean_q: 33.924130, mean_eps: 0.250480\n",
            " 8503/10000: episode: 214, duration: 0.779s, episode steps: 108, steps per second: 139, episode reward: 108.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.463 [0.000, 1.000],  loss: 8.315883, mae: 17.435537, mean_q: 34.288495, mean_eps: 0.239635\n",
            " 8659/10000: episode: 215, duration: 1.026s, episode steps: 156, steps per second: 152, episode reward: 156.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  loss: 7.941466, mae: 17.672892, mean_q: 34.869850, mean_eps: 0.227755\n",
            " 8794/10000: episode: 216, duration: 0.947s, episode steps: 135, steps per second: 143, episode reward: 135.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  loss: 8.391009, mae: 17.913955, mean_q: 35.323005, mean_eps: 0.214660\n",
            " 8994/10000: episode: 217, duration: 1.672s, episode steps: 200, steps per second: 120, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  loss: 10.164829, mae: 18.139718, mean_q: 35.600626, mean_eps: 0.199585\n",
            " 9190/10000: episode: 218, duration: 1.711s, episode steps: 196, steps per second: 115, episode reward: 196.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.536 [0.000, 1.000],  loss: 8.442815, mae: 18.349489, mean_q: 36.218231, mean_eps: 0.181765\n",
            " 9324/10000: episode: 219, duration: 0.861s, episode steps: 134, steps per second: 156, episode reward: 134.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  loss: 9.040308, mae: 18.631443, mean_q: 36.753513, mean_eps: 0.166915\n",
            " 9428/10000: episode: 220, duration: 0.690s, episode steps: 104, steps per second: 151, episode reward: 104.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  loss: 8.944629, mae: 18.734430, mean_q: 37.033988, mean_eps: 0.156205\n",
            " 9543/10000: episode: 221, duration: 0.755s, episode steps: 115, steps per second: 152, episode reward: 115.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.461 [0.000, 1.000],  loss: 9.096156, mae: 18.967910, mean_q: 37.437758, mean_eps: 0.146350\n",
            " 9708/10000: episode: 222, duration: 1.158s, episode steps: 165, steps per second: 143, episode reward: 165.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.479 [0.000, 1.000],  loss: 9.507880, mae: 19.153399, mean_q: 37.821042, mean_eps: 0.133750\n",
            " 9818/10000: episode: 223, duration: 0.721s, episode steps: 110, steps per second: 153, episode reward: 110.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.464 [0.000, 1.000],  loss: 9.402911, mae: 19.231755, mean_q: 38.032800, mean_eps: 0.121375\n",
            "done, took 88.452 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# summarize the history for number  of episode steps\n",
        "plt.plot(history.history['nb_episode_steps'])\n",
        "plt.ylabel('nb_episode_steps')\n",
        "plt.xlabel('episodes')\n",
        "plt.show()\n",
        "\n",
        "dqn.test(env, nb_episodes=20, visualize=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 661
        },
        "id": "bkKOscyvlbDf",
        "outputId": "b0e30f62-0859-4cd5-d72a-17921ab83c83"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing for 20 episodes ...\n",
            "Episode 1: reward: 200.000, steps: 200\n",
            "Episode 2: reward: 200.000, steps: 200\n",
            "Episode 3: reward: 200.000, steps: 200\n",
            "Episode 4: reward: 200.000, steps: 200\n",
            "Episode 5: reward: 200.000, steps: 200\n",
            "Episode 6: reward: 200.000, steps: 200\n",
            "Episode 7: reward: 200.000, steps: 200\n",
            "Episode 8: reward: 200.000, steps: 200\n",
            "Episode 9: reward: 176.000, steps: 176\n",
            "Episode 10: reward: 200.000, steps: 200\n",
            "Episode 11: reward: 200.000, steps: 200\n",
            "Episode 12: reward: 186.000, steps: 186\n",
            "Episode 13: reward: 200.000, steps: 200\n",
            "Episode 14: reward: 183.000, steps: 183\n",
            "Episode 15: reward: 200.000, steps: 200\n",
            "Episode 16: reward: 200.000, steps: 200\n",
            "Episode 17: reward: 200.000, steps: 200\n",
            "Episode 18: reward: 200.000, steps: 200\n",
            "Episode 19: reward: 200.000, steps: 200\n",
            "Episode 20: reward: 200.000, steps: 200\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f9a4c85d520>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    }
  ]
}