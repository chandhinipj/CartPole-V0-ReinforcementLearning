{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chandhinipj/CartPole-V0-ReinforcementLearning/blob/main/PredictionChallenge3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VKK5DA390wRe"
      },
      "source": [
        "# Skeleton Code for Prediction Challenge 3\n",
        "Below is partial code to get you started on prediction challenge 3. You need to select values for the parameters that have question marks."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install keras-rl2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DNGTghHEhyW4",
        "outputId": "547c7896-df78-46bb-ce0b-d60af33a42ad"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: keras-rl2 in /usr/local/lib/python3.9/dist-packages (1.0.5)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.9/dist-packages (from keras-rl2) (2.11.0)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (23.3.3)\n",
            "Requirement already satisfied: tensorboard<2.12,>=2.11 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (2.11.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.12,>=2.11.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (2.11.0)\n",
            "Requirement already satisfied: keras<2.12,>=2.11.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (2.11.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (15.0.6.1)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (0.4.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (1.15.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (2.2.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (23.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (1.15.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (3.3.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (3.1.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (1.4.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (63.4.3)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (1.51.3)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (3.19.6)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (1.22.4)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (0.31.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (4.5.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (1.6.3)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.9/dist-packages (from astunparse>=1.6.0->tensorflow->keras-rl2) (0.38.4)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow->keras-rl2) (1.8.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow->keras-rl2) (0.4.6)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow->keras-rl2) (3.4.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow->keras-rl2) (2.25.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow->keras-rl2) (0.6.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow->keras-rl2) (2.2.3)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow->keras-rl2) (2.16.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow->keras-rl2) (4.9)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow->keras-rl2) (0.2.8)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow->keras-rl2) (5.3.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow->keras-rl2) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.9/dist-packages (from markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow->keras-rl2) (6.0.0)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow->keras-rl2) (4.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow->keras-rl2) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow->keras-rl2) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow->keras-rl2) (2022.12.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.9/dist-packages (from werkzeug>=1.0.1->tensorboard<2.12,>=2.11->tensorflow->keras-rl2) (2.1.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow->keras-rl2) (3.15.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.9/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow->keras-rl2) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.9/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow->keras-rl2) (3.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gym"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eQ_dXOCKh0jV",
        "outputId": "44decb80-7a77-462a-fe2e-14c4a50dfbd2"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.9/dist-packages (0.25.2)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.9/dist-packages (from gym) (6.0.0)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.9/dist-packages (from gym) (2.2.1)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.9/dist-packages (from gym) (1.22.4)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.9/dist-packages (from gym) (0.0.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=4.8.0->gym) (3.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load the gym module\n",
        "import gym\n",
        "import matplotlib.pyplot as plt\n",
        "# import the usual Keras modules for creating deep neural networks\n",
        "from keras import Sequential\n",
        "from keras.layers import Input, Flatten, Dense\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "ENV_NAME = 'CartPole-v0'\n",
        "env = gym.make(ENV_NAME)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "75Phf54nh2y_",
        "outputId": "62a688e3-10a0-4f8d-9c1c-c88cdd1668fc"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/gym/envs/registration.py:593: UserWarning: \u001b[33mWARN: The environment CartPole-v0 is out of date. You should consider upgrading to version `v1`.\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.9/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import rl\n",
        "from rl.memory import SequentialMemory  # import the exerience replay buffer module\n",
        "from rl.policy import LinearAnnealedPolicy, EpsGreedyQPolicy  # import the policy\n",
        "from rl.agents.dqn import DQNAgent      # import the DQN agent"
      ],
      "metadata": {
        "id": "KsGYDOrDh58e"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q-Network\n",
        "model = Sequential()\n",
        "model.add(Input(shape=(1,env.observation_space.shape[0])))  # The input is 1 observation vector, and the number of observations in that vector \n",
        "model.add(Flatten())\n",
        "# add extra layers here\n",
        "model.add(Dense(16, activation='relu'))\n",
        "# model.add(Dense(32, activation='relu'))\n",
        "# model.add(Dense(64, activation='relu'))\n",
        "# model.add(Dense(128, activation='relu'))\n",
        "\n",
        "model.add(Dense(env.action_space.n, activation='linear'))   # the output is the number of actions in the action space\n",
        "print(model.summary())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i5dvD8xTio0U",
        "outputId": "b53aaf10-e220-4c38-9cbe-068f602e1d97"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten (Flatten)           (None, 4)                 0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 16)                80        \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 2)                 34        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 114\n",
            "Trainable params: 114\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R8ZiiRbxlH2D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e74c0e6a-7456-4204-e16a-40fcd82ad7af"
      },
      "source": [
        "memory = SequentialMemory(limit=100000, window_length=1)\n",
        "\n",
        "# define the policy\n",
        "policy =  LinearAnnealedPolicy(inner_policy=EpsGreedyQPolicy(), \n",
        "                               attr='eps',            \n",
        "                               value_max=1.0,\n",
        "                               value_min=0.1, \n",
        "                               value_test=.05,\n",
        "                               nb_steps=10000)\n",
        "\n",
        "\n",
        "# define the agent\n",
        "dqn = DQNAgent(model=model, \n",
        "               nb_actions=env.action_space.n,\n",
        "               memory=memory,\n",
        "               nb_steps_warmup=100,\n",
        "               target_model_update=1e-2, \n",
        "               policy=policy) \n",
        "\n",
        "dqn.compile(Adam(lr=1e-3), metrics=['mae'])\n",
        "\n",
        "history = dqn.fit(env, nb_steps=10000, visualize=False, verbose=2)\n",
        "\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/optimizers/optimizer_v2/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training for 10000 steps ...\n",
            "   22/10000: episode: 1, duration: 0.083s, episode steps:  22, steps per second: 264, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   50/10000: episode: 2, duration: 0.020s, episode steps:  28, steps per second: 1396, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   67/10000: episode: 3, duration: 0.012s, episode steps:  17, steps per second: 1371, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.294 [0.000, 1.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   96/10000: episode: 4, duration: 0.021s, episode steps:  29, steps per second: 1403, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.483 [0.000, 1.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates=self.state_updates,\n",
            "/usr/local/lib/python3.9/dist-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates=self.state_updates,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  116/10000: episode: 5, duration: 0.726s, episode steps:  20, steps per second:  28, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.650 [0.000, 1.000],  loss: 0.509598, mae: 0.536816, mean_q: 0.087398, mean_eps: 0.990280\n",
            "  137/10000: episode: 6, duration: 0.132s, episode steps:  21, steps per second: 160, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  loss: 0.472393, mae: 0.531854, mean_q: 0.134183, mean_eps: 0.988660\n",
            "  166/10000: episode: 7, duration: 0.199s, episode steps:  29, steps per second: 146, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.552 [0.000, 1.000],  loss: 0.423490, mae: 0.537997, mean_q: 0.201948, mean_eps: 0.986410\n",
            "  195/10000: episode: 8, duration: 0.170s, episode steps:  29, steps per second: 171, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.552 [0.000, 1.000],  loss: 0.384691, mae: 0.551385, mean_q: 0.277871, mean_eps: 0.983800\n",
            "  227/10000: episode: 9, duration: 0.210s, episode steps:  32, steps per second: 152, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.338347, mae: 0.577399, mean_q: 0.398853, mean_eps: 0.981055\n",
            "  238/10000: episode: 10, duration: 0.069s, episode steps:  11, steps per second: 159, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.727 [0.000, 1.000],  loss: 0.312677, mae: 0.590535, mean_q: 0.466370, mean_eps: 0.979120\n",
            "  251/10000: episode: 11, duration: 0.085s, episode steps:  13, steps per second: 153, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.308 [0.000, 1.000],  loss: 0.292441, mae: 0.609054, mean_q: 0.532507, mean_eps: 0.978040\n",
            "  302/10000: episode: 12, duration: 0.414s, episode steps:  51, steps per second: 123, episode reward: 51.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.412 [0.000, 1.000],  loss: 0.231493, mae: 0.666981, mean_q: 0.732252, mean_eps: 0.975160\n",
            "  324/10000: episode: 13, duration: 0.196s, episode steps:  22, steps per second: 112, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.636 [0.000, 1.000],  loss: 0.173224, mae: 0.765696, mean_q: 1.030254, mean_eps: 0.971875\n",
            "  347/10000: episode: 14, duration: 0.207s, episode steps:  23, steps per second: 111, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.478 [0.000, 1.000],  loss: 0.141874, mae: 0.804058, mean_q: 1.168697, mean_eps: 0.969850\n",
            "  369/10000: episode: 15, duration: 0.199s, episode steps:  22, steps per second: 111, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.409 [0.000, 1.000],  loss: 0.122916, mae: 0.870893, mean_q: 1.356186, mean_eps: 0.967825\n",
            "  391/10000: episode: 16, duration: 0.219s, episode steps:  22, steps per second: 101, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  loss: 0.103669, mae: 0.932747, mean_q: 1.521026, mean_eps: 0.965845\n",
            "  405/10000: episode: 17, duration: 0.130s, episode steps:  14, steps per second: 107, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.357 [0.000, 1.000],  loss: 0.106141, mae: 0.990630, mean_q: 1.657004, mean_eps: 0.964225\n",
            "  415/10000: episode: 18, duration: 0.100s, episode steps:  10, steps per second: 100, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.900 [0.000, 1.000],  loss: 0.093087, mae: 1.020218, mean_q: 1.752476, mean_eps: 0.963145\n",
            "  435/10000: episode: 19, duration: 0.179s, episode steps:  20, steps per second: 112, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.650 [0.000, 1.000],  loss: 0.109672, mae: 1.093901, mean_q: 1.893539, mean_eps: 0.961795\n",
            "  451/10000: episode: 20, duration: 0.149s, episode steps:  16, steps per second: 108, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.312 [0.000, 1.000],  loss: 0.121858, mae: 1.167770, mean_q: 2.037081, mean_eps: 0.960175\n",
            "  469/10000: episode: 21, duration: 0.160s, episode steps:  18, steps per second: 112, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 0.128455, mae: 1.223973, mean_q: 2.149879, mean_eps: 0.958645\n",
            "  494/10000: episode: 22, duration: 0.243s, episode steps:  25, steps per second: 103, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  loss: 0.153144, mae: 1.327280, mean_q: 2.339371, mean_eps: 0.956710\n",
            "  516/10000: episode: 23, duration: 0.199s, episode steps:  22, steps per second: 111, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.682 [0.000, 1.000],  loss: 0.152721, mae: 1.403815, mean_q: 2.509069, mean_eps: 0.954595\n",
            "  534/10000: episode: 24, duration: 0.166s, episode steps:  18, steps per second: 108, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.444 [0.000, 1.000],  loss: 0.153926, mae: 1.477519, mean_q: 2.667926, mean_eps: 0.952795\n",
            "  544/10000: episode: 25, duration: 0.101s, episode steps:  10, steps per second:  99, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.900 [0.000, 1.000],  loss: 0.206722, mae: 1.545085, mean_q: 2.788731, mean_eps: 0.951535\n",
            "  562/10000: episode: 26, duration: 0.130s, episode steps:  18, steps per second: 139, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 0.191308, mae: 1.616498, mean_q: 2.917197, mean_eps: 0.950275\n",
            "  577/10000: episode: 27, duration: 0.099s, episode steps:  15, steps per second: 151, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 0.280356, mae: 1.690382, mean_q: 3.044478, mean_eps: 0.948790\n",
            "  595/10000: episode: 28, duration: 0.114s, episode steps:  18, steps per second: 158, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 0.195743, mae: 1.727470, mean_q: 3.137644, mean_eps: 0.947305\n",
            "  614/10000: episode: 29, duration: 0.123s, episode steps:  19, steps per second: 155, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.421 [0.000, 1.000],  loss: 0.280812, mae: 1.834512, mean_q: 3.330551, mean_eps: 0.945640\n",
            "  630/10000: episode: 30, duration: 0.106s, episode steps:  16, steps per second: 151, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.312 [0.000, 1.000],  loss: 0.192899, mae: 1.855377, mean_q: 3.429614, mean_eps: 0.944065\n",
            "  641/10000: episode: 31, duration: 0.073s, episode steps:  11, steps per second: 151, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.818 [0.000, 1.000],  loss: 0.241756, mae: 1.931758, mean_q: 3.534318, mean_eps: 0.942850\n",
            "  658/10000: episode: 32, duration: 0.112s, episode steps:  17, steps per second: 152, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.647 [0.000, 1.000],  loss: 0.208298, mae: 1.976776, mean_q: 3.682550, mean_eps: 0.941590\n",
            "  673/10000: episode: 33, duration: 0.094s, episode steps:  15, steps per second: 159, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  loss: 0.312421, mae: 2.072971, mean_q: 3.874030, mean_eps: 0.940150\n",
            "  689/10000: episode: 34, duration: 0.107s, episode steps:  16, steps per second: 149, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.625 [0.000, 1.000],  loss: 0.398425, mae: 2.152923, mean_q: 3.965351, mean_eps: 0.938755\n",
            "  701/10000: episode: 35, duration: 0.081s, episode steps:  12, steps per second: 148, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.250 [0.000, 1.000],  loss: 0.316716, mae: 2.187822, mean_q: 4.028965, mean_eps: 0.937495\n",
            "  725/10000: episode: 36, duration: 0.149s, episode steps:  24, steps per second: 161, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.386473, mae: 2.254922, mean_q: 4.137426, mean_eps: 0.935875\n",
            "  744/10000: episode: 37, duration: 0.130s, episode steps:  19, steps per second: 146, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.684 [0.000, 1.000],  loss: 0.441010, mae: 2.363451, mean_q: 4.342692, mean_eps: 0.933940\n",
            "  786/10000: episode: 38, duration: 0.269s, episode steps:  42, steps per second: 156, episode reward: 42.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.405 [0.000, 1.000],  loss: 0.392430, mae: 2.439291, mean_q: 4.522115, mean_eps: 0.931195\n",
            "  812/10000: episode: 39, duration: 0.163s, episode steps:  26, steps per second: 159, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.385 [0.000, 1.000],  loss: 0.675613, mae: 2.633365, mean_q: 4.769183, mean_eps: 0.928135\n",
            "  829/10000: episode: 40, duration: 0.127s, episode steps:  17, steps per second: 134, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.176 [0.000, 1.000],  loss: 0.638729, mae: 2.691182, mean_q: 4.834296, mean_eps: 0.926200\n",
            "  842/10000: episode: 41, duration: 0.099s, episode steps:  13, steps per second: 131, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.846 [0.000, 1.000],  loss: 0.694362, mae: 2.748284, mean_q: 4.940437, mean_eps: 0.924850\n",
            "  862/10000: episode: 42, duration: 0.127s, episode steps:  20, steps per second: 158, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  loss: 0.609291, mae: 2.791577, mean_q: 5.061001, mean_eps: 0.923365\n",
            "  877/10000: episode: 43, duration: 0.093s, episode steps:  15, steps per second: 162, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 0.449350, mae: 2.830632, mean_q: 5.241853, mean_eps: 0.921790\n",
            "  898/10000: episode: 44, duration: 0.143s, episode steps:  21, steps per second: 147, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  loss: 0.473138, mae: 2.888071, mean_q: 5.429491, mean_eps: 0.920170\n",
            "  911/10000: episode: 45, duration: 0.084s, episode steps:  13, steps per second: 155, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: 0.338109, mae: 2.929568, mean_q: 5.538022, mean_eps: 0.918640\n",
            "  929/10000: episode: 46, duration: 0.120s, episode steps:  18, steps per second: 150, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.278 [0.000, 1.000],  loss: 0.692938, mae: 3.051656, mean_q: 5.619652, mean_eps: 0.917245\n",
            "  950/10000: episode: 47, duration: 0.128s, episode steps:  21, steps per second: 164, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.429 [0.000, 1.000],  loss: 0.594625, mae: 3.093001, mean_q: 5.668045, mean_eps: 0.915490\n",
            "  970/10000: episode: 48, duration: 0.128s, episode steps:  20, steps per second: 156, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.450 [0.000, 1.000],  loss: 0.457098, mae: 3.116264, mean_q: 5.842694, mean_eps: 0.913645\n",
            " 1014/10000: episode: 49, duration: 0.299s, episode steps:  44, steps per second: 147, episode reward: 44.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.477 [0.000, 1.000],  loss: 0.606741, mae: 3.274742, mean_q: 6.134743, mean_eps: 0.910765\n",
            " 1039/10000: episode: 50, duration: 0.156s, episode steps:  25, steps per second: 160, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  loss: 0.618299, mae: 3.385603, mean_q: 6.373336, mean_eps: 0.907660\n",
            " 1056/10000: episode: 51, duration: 0.113s, episode steps:  17, steps per second: 150, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.588 [0.000, 1.000],  loss: 0.820322, mae: 3.497638, mean_q: 6.523714, mean_eps: 0.905770\n",
            " 1075/10000: episode: 52, duration: 0.128s, episode steps:  19, steps per second: 148, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  loss: 0.857900, mae: 3.567251, mean_q: 6.563021, mean_eps: 0.904150\n",
            " 1089/10000: episode: 53, duration: 0.098s, episode steps:  14, steps per second: 143, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.834612, mae: 3.627019, mean_q: 6.712464, mean_eps: 0.902665\n",
            " 1124/10000: episode: 54, duration: 0.219s, episode steps:  35, steps per second: 160, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.514 [0.000, 1.000],  loss: 0.780541, mae: 3.681560, mean_q: 6.877597, mean_eps: 0.900460\n",
            " 1155/10000: episode: 55, duration: 0.208s, episode steps:  31, steps per second: 149, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.419 [0.000, 1.000],  loss: 0.759613, mae: 3.786699, mean_q: 7.074875, mean_eps: 0.897490\n",
            " 1195/10000: episode: 56, duration: 0.246s, episode steps:  40, steps per second: 162, episode reward: 40.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.475 [0.000, 1.000],  loss: 0.744136, mae: 3.901060, mean_q: 7.404554, mean_eps: 0.894295\n",
            " 1211/10000: episode: 57, duration: 0.099s, episode steps:  16, steps per second: 161, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.438 [0.000, 1.000],  loss: 1.063959, mae: 4.029113, mean_q: 7.476907, mean_eps: 0.891775\n",
            " 1231/10000: episode: 58, duration: 0.135s, episode steps:  20, steps per second: 149, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.650 [0.000, 1.000],  loss: 0.970568, mae: 4.069112, mean_q: 7.609825, mean_eps: 0.890155\n",
            " 1263/10000: episode: 59, duration: 0.201s, episode steps:  32, steps per second: 159, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.438 [0.000, 1.000],  loss: 1.094676, mae: 4.162732, mean_q: 7.727432, mean_eps: 0.887815\n",
            " 1290/10000: episode: 60, duration: 0.171s, episode steps:  27, steps per second: 158, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.556 [0.000, 1.000],  loss: 0.907394, mae: 4.248439, mean_q: 8.057085, mean_eps: 0.885160\n",
            " 1320/10000: episode: 61, duration: 0.187s, episode steps:  30, steps per second: 160, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 1.002631, mae: 4.348084, mean_q: 8.253034, mean_eps: 0.882595\n",
            " 1351/10000: episode: 62, duration: 0.203s, episode steps:  31, steps per second: 153, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.452 [0.000, 1.000],  loss: 0.885949, mae: 4.440541, mean_q: 8.419380, mean_eps: 0.879850\n",
            " 1370/10000: episode: 63, duration: 0.119s, episode steps:  19, steps per second: 160, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.579 [0.000, 1.000],  loss: 0.820146, mae: 4.516610, mean_q: 8.621352, mean_eps: 0.877600\n",
            " 1415/10000: episode: 64, duration: 0.274s, episode steps:  45, steps per second: 164, episode reward: 45.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.489 [0.000, 1.000],  loss: 1.202271, mae: 4.659665, mean_q: 8.794517, mean_eps: 0.874720\n",
            " 1427/10000: episode: 65, duration: 0.089s, episode steps:  12, steps per second: 135, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 1.327052, mae: 4.748949, mean_q: 8.990801, mean_eps: 0.872155\n",
            " 1471/10000: episode: 66, duration: 0.295s, episode steps:  44, steps per second: 149, episode reward: 44.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.455 [0.000, 1.000],  loss: 1.101633, mae: 4.823664, mean_q: 9.157708, mean_eps: 0.869635\n",
            " 1504/10000: episode: 67, duration: 0.222s, episode steps:  33, steps per second: 149, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.455 [0.000, 1.000],  loss: 1.165526, mae: 4.955507, mean_q: 9.395207, mean_eps: 0.866170\n",
            " 1519/10000: episode: 68, duration: 0.095s, episode steps:  15, steps per second: 158, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 1.259044, mae: 5.043978, mean_q: 9.616981, mean_eps: 0.864010\n",
            " 1548/10000: episode: 69, duration: 0.188s, episode steps:  29, steps per second: 154, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.586 [0.000, 1.000],  loss: 1.279522, mae: 5.107182, mean_q: 9.695130, mean_eps: 0.862030\n",
            " 1576/10000: episode: 70, duration: 0.177s, episode steps:  28, steps per second: 158, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 1.598751, mae: 5.221683, mean_q: 9.801741, mean_eps: 0.859465\n",
            " 1620/10000: episode: 71, duration: 0.272s, episode steps:  44, steps per second: 162, episode reward: 44.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.477 [0.000, 1.000],  loss: 1.403804, mae: 5.293933, mean_q: 10.000355, mean_eps: 0.856225\n",
            " 1635/10000: episode: 72, duration: 0.093s, episode steps:  15, steps per second: 161, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  loss: 1.323738, mae: 5.385091, mean_q: 10.323747, mean_eps: 0.853570\n",
            " 1650/10000: episode: 73, duration: 0.096s, episode steps:  15, steps per second: 157, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  loss: 2.009926, mae: 5.484731, mean_q: 10.343243, mean_eps: 0.852220\n",
            " 1694/10000: episode: 74, duration: 0.279s, episode steps:  44, steps per second: 158, episode reward: 44.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.477 [0.000, 1.000],  loss: 1.788608, mae: 5.552747, mean_q: 10.463352, mean_eps: 0.849565\n",
            " 1711/10000: episode: 75, duration: 0.131s, episode steps:  17, steps per second: 130, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.588 [0.000, 1.000],  loss: 1.779364, mae: 5.608446, mean_q: 10.549479, mean_eps: 0.846820\n",
            " 1746/10000: episode: 76, duration: 0.226s, episode steps:  35, steps per second: 155, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  loss: 1.828610, mae: 5.697149, mean_q: 10.705933, mean_eps: 0.844480\n",
            " 1762/10000: episode: 77, duration: 0.107s, episode steps:  16, steps per second: 149, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.688 [0.000, 1.000],  loss: 1.682453, mae: 5.767235, mean_q: 10.969339, mean_eps: 0.842185\n",
            " 1787/10000: episode: 78, duration: 0.165s, episode steps:  25, steps per second: 152, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  loss: 2.245719, mae: 5.863086, mean_q: 10.940846, mean_eps: 0.840340\n",
            " 1804/10000: episode: 79, duration: 0.109s, episode steps:  17, steps per second: 156, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  loss: 1.766053, mae: 5.842320, mean_q: 11.007850, mean_eps: 0.838450\n",
            " 1848/10000: episode: 80, duration: 0.285s, episode steps:  44, steps per second: 154, episode reward: 44.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.455 [0.000, 1.000],  loss: 1.690916, mae: 5.948662, mean_q: 11.319101, mean_eps: 0.835705\n",
            " 1870/10000: episode: 81, duration: 0.150s, episode steps:  22, steps per second: 146, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 2.030682, mae: 6.056594, mean_q: 11.558975, mean_eps: 0.832735\n",
            " 1895/10000: episode: 82, duration: 0.165s, episode steps:  25, steps per second: 151, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  loss: 2.339196, mae: 6.132215, mean_q: 11.543072, mean_eps: 0.830620\n",
            " 1910/10000: episode: 83, duration: 0.094s, episode steps:  15, steps per second: 160, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 1.794444, mae: 6.167556, mean_q: 11.644103, mean_eps: 0.828820\n",
            " 1968/10000: episode: 84, duration: 0.364s, episode steps:  58, steps per second: 159, episode reward: 58.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.552 [0.000, 1.000],  loss: 2.611982, mae: 6.302756, mean_q: 11.804331, mean_eps: 0.825535\n",
            " 2013/10000: episode: 85, duration: 0.279s, episode steps:  45, steps per second: 161, episode reward: 45.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 1.994682, mae: 6.374797, mean_q: 12.109585, mean_eps: 0.820900\n",
            " 2048/10000: episode: 86, duration: 0.223s, episode steps:  35, steps per second: 157, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.457 [0.000, 1.000],  loss: 2.392238, mae: 6.469878, mean_q: 12.215250, mean_eps: 0.817300\n",
            " 2101/10000: episode: 87, duration: 0.401s, episode steps:  53, steps per second: 132, episode reward: 53.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.509 [0.000, 1.000],  loss: 2.074184, mae: 6.575304, mean_q: 12.540498, mean_eps: 0.813340\n",
            " 2132/10000: episode: 88, duration: 0.281s, episode steps:  31, steps per second: 110, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.548 [0.000, 1.000],  loss: 2.257647, mae: 6.700326, mean_q: 12.803765, mean_eps: 0.809560\n",
            " 2153/10000: episode: 89, duration: 0.196s, episode steps:  21, steps per second: 107, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  loss: 2.191851, mae: 6.771859, mean_q: 12.871589, mean_eps: 0.807220\n",
            " 2214/10000: episode: 90, duration: 0.526s, episode steps:  61, steps per second: 116, episode reward: 61.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.508 [0.000, 1.000],  loss: 2.625687, mae: 6.912885, mean_q: 13.110147, mean_eps: 0.803530\n",
            " 2242/10000: episode: 91, duration: 0.262s, episode steps:  28, steps per second: 107, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.429 [0.000, 1.000],  loss: 2.430573, mae: 7.000433, mean_q: 13.348570, mean_eps: 0.799525\n",
            " 2256/10000: episode: 92, duration: 0.144s, episode steps:  14, steps per second:  98, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.429 [0.000, 1.000],  loss: 1.441902, mae: 6.997899, mean_q: 13.535346, mean_eps: 0.797635\n",
            " 2284/10000: episode: 93, duration: 0.291s, episode steps:  28, steps per second:  96, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 2.181103, mae: 7.120924, mean_q: 13.683640, mean_eps: 0.795745\n",
            " 2318/10000: episode: 94, duration: 0.317s, episode steps:  34, steps per second: 107, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  loss: 2.124237, mae: 7.174478, mean_q: 13.845202, mean_eps: 0.792955\n",
            " 2386/10000: episode: 95, duration: 0.538s, episode steps:  68, steps per second: 126, episode reward: 68.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 2.395544, mae: 7.328058, mean_q: 14.096827, mean_eps: 0.788365\n",
            " 2413/10000: episode: 96, duration: 0.197s, episode steps:  27, steps per second: 137, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  loss: 2.367774, mae: 7.435196, mean_q: 14.283375, mean_eps: 0.784090\n",
            " 2441/10000: episode: 97, duration: 0.199s, episode steps:  28, steps per second: 141, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.429 [0.000, 1.000],  loss: 3.264215, mae: 7.583890, mean_q: 14.431964, mean_eps: 0.781615\n",
            " 2518/10000: episode: 98, duration: 0.468s, episode steps:  77, steps per second: 165, episode reward: 77.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.532 [0.000, 1.000],  loss: 2.851436, mae: 7.625870, mean_q: 14.548759, mean_eps: 0.776890\n",
            " 2529/10000: episode: 99, duration: 0.074s, episode steps:  11, steps per second: 148, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.727 [0.000, 1.000],  loss: 2.724691, mae: 7.738096, mean_q: 14.894530, mean_eps: 0.772930\n",
            " 2550/10000: episode: 100, duration: 0.137s, episode steps:  21, steps per second: 153, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  loss: 4.038716, mae: 7.904731, mean_q: 14.907804, mean_eps: 0.771490\n",
            " 2584/10000: episode: 101, duration: 0.214s, episode steps:  34, steps per second: 159, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  loss: 2.619181, mae: 7.831506, mean_q: 14.955174, mean_eps: 0.769015\n",
            " 2597/10000: episode: 102, duration: 0.081s, episode steps:  13, steps per second: 161, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.692 [0.000, 1.000],  loss: 2.919413, mae: 7.886076, mean_q: 15.161954, mean_eps: 0.766900\n",
            " 2613/10000: episode: 103, duration: 0.109s, episode steps:  16, steps per second: 146, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.562 [0.000, 1.000],  loss: 2.276992, mae: 7.892589, mean_q: 15.273240, mean_eps: 0.765595\n",
            " 2646/10000: episode: 104, duration: 0.202s, episode steps:  33, steps per second: 163, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.576 [0.000, 1.000],  loss: 3.007676, mae: 8.015315, mean_q: 15.370672, mean_eps: 0.763390\n",
            " 2670/10000: episode: 105, duration: 0.149s, episode steps:  24, steps per second: 162, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.458 [0.000, 1.000],  loss: 3.370900, mae: 8.061636, mean_q: 15.458059, mean_eps: 0.760825\n",
            " 2703/10000: episode: 106, duration: 0.218s, episode steps:  33, steps per second: 151, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  loss: 3.178358, mae: 8.149895, mean_q: 15.600808, mean_eps: 0.758260\n",
            " 2746/10000: episode: 107, duration: 0.269s, episode steps:  43, steps per second: 160, episode reward: 43.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.558 [0.000, 1.000],  loss: 4.208064, mae: 8.304189, mean_q: 15.753110, mean_eps: 0.754840\n",
            " 2759/10000: episode: 108, duration: 0.085s, episode steps:  13, steps per second: 153, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.615 [0.000, 1.000],  loss: 3.840788, mae: 8.347192, mean_q: 15.775367, mean_eps: 0.752320\n",
            " 2778/10000: episode: 109, duration: 0.131s, episode steps:  19, steps per second: 145, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 3.402555, mae: 8.276278, mean_q: 15.759537, mean_eps: 0.750880\n",
            " 2801/10000: episode: 110, duration: 0.148s, episode steps:  23, steps per second: 155, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.478 [0.000, 1.000],  loss: 3.161494, mae: 8.380900, mean_q: 16.084682, mean_eps: 0.748990\n",
            " 2825/10000: episode: 111, duration: 0.154s, episode steps:  24, steps per second: 156, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.542 [0.000, 1.000],  loss: 3.033237, mae: 8.366222, mean_q: 16.092808, mean_eps: 0.746875\n",
            " 2867/10000: episode: 112, duration: 0.267s, episode steps:  42, steps per second: 157, episode reward: 42.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 3.422056, mae: 8.488199, mean_q: 16.280599, mean_eps: 0.743905\n",
            " 2907/10000: episode: 113, duration: 0.245s, episode steps:  40, steps per second: 163, episode reward: 40.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.525 [0.000, 1.000],  loss: 3.302804, mae: 8.593311, mean_q: 16.555719, mean_eps: 0.740215\n",
            " 2927/10000: episode: 114, duration: 0.123s, episode steps:  20, steps per second: 163, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 3.652072, mae: 8.695518, mean_q: 16.698551, mean_eps: 0.737515\n",
            " 3034/10000: episode: 115, duration: 0.673s, episode steps: 107, steps per second: 159, episode reward: 107.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.514 [0.000, 1.000],  loss: 3.944805, mae: 8.797388, mean_q: 16.841561, mean_eps: 0.731800\n",
            " 3049/10000: episode: 116, duration: 0.103s, episode steps:  15, steps per second: 146, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  loss: 3.286788, mae: 8.869200, mean_q: 17.111009, mean_eps: 0.726310\n",
            " 3091/10000: episode: 117, duration: 0.266s, episode steps:  42, steps per second: 158, episode reward: 42.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 3.219713, mae: 8.958218, mean_q: 17.404902, mean_eps: 0.723745\n",
            " 3158/10000: episode: 118, duration: 0.420s, episode steps:  67, steps per second: 159, episode reward: 67.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.507 [0.000, 1.000],  loss: 3.363510, mae: 9.098104, mean_q: 17.607660, mean_eps: 0.718840\n",
            " 3183/10000: episode: 119, duration: 0.167s, episode steps:  25, steps per second: 150, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  loss: 3.913561, mae: 9.246611, mean_q: 17.809549, mean_eps: 0.714700\n",
            " 3195/10000: episode: 120, duration: 0.087s, episode steps:  12, steps per second: 137, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 3.445547, mae: 9.277035, mean_q: 17.846553, mean_eps: 0.713035\n",
            " 3228/10000: episode: 121, duration: 0.208s, episode steps:  33, steps per second: 158, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.515 [0.000, 1.000],  loss: 3.517417, mae: 9.295439, mean_q: 18.043206, mean_eps: 0.711010\n",
            " 3270/10000: episode: 122, duration: 0.272s, episode steps:  42, steps per second: 154, episode reward: 42.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  loss: 5.359990, mae: 9.497669, mean_q: 18.042047, mean_eps: 0.707635\n",
            " 3306/10000: episode: 123, duration: 0.239s, episode steps:  36, steps per second: 151, episode reward: 36.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.472 [0.000, 1.000],  loss: 3.918600, mae: 9.430417, mean_q: 18.163526, mean_eps: 0.704125\n",
            " 3351/10000: episode: 124, duration: 0.307s, episode steps:  45, steps per second: 147, episode reward: 45.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.489 [0.000, 1.000],  loss: 4.659365, mae: 9.591727, mean_q: 18.480100, mean_eps: 0.700480\n",
            " 3374/10000: episode: 125, duration: 0.153s, episode steps:  23, steps per second: 151, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.609 [0.000, 1.000],  loss: 2.981195, mae: 9.592932, mean_q: 18.590399, mean_eps: 0.697420\n",
            " 3445/10000: episode: 126, duration: 0.459s, episode steps:  71, steps per second: 155, episode reward: 71.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.521 [0.000, 1.000],  loss: 4.108992, mae: 9.734947, mean_q: 18.796314, mean_eps: 0.693190\n",
            " 3534/10000: episode: 127, duration: 0.560s, episode steps:  89, steps per second: 159, episode reward: 89.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.506 [0.000, 1.000],  loss: 4.101621, mae: 9.915701, mean_q: 19.222969, mean_eps: 0.685990\n",
            " 3571/10000: episode: 128, duration: 0.242s, episode steps:  37, steps per second: 153, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.405 [0.000, 1.000],  loss: 4.780163, mae: 10.113169, mean_q: 19.442726, mean_eps: 0.680320\n",
            " 3596/10000: episode: 129, duration: 0.159s, episode steps:  25, steps per second: 157, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  loss: 5.283946, mae: 10.162151, mean_q: 19.451423, mean_eps: 0.677530\n",
            " 3631/10000: episode: 130, duration: 0.224s, episode steps:  35, steps per second: 156, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.486 [0.000, 1.000],  loss: 4.590148, mae: 10.200039, mean_q: 19.635911, mean_eps: 0.674830\n",
            " 3674/10000: episode: 131, duration: 0.293s, episode steps:  43, steps per second: 147, episode reward: 43.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.488 [0.000, 1.000],  loss: 3.918703, mae: 10.205125, mean_q: 19.819361, mean_eps: 0.671320\n",
            " 3713/10000: episode: 132, duration: 0.249s, episode steps:  39, steps per second: 156, episode reward: 39.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.487 [0.000, 1.000],  loss: 4.346949, mae: 10.360368, mean_q: 20.064725, mean_eps: 0.667630\n",
            " 3735/10000: episode: 133, duration: 0.139s, episode steps:  22, steps per second: 158, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.455 [0.000, 1.000],  loss: 4.483242, mae: 10.365794, mean_q: 20.048527, mean_eps: 0.664885\n",
            " 3763/10000: episode: 134, duration: 0.190s, episode steps:  28, steps per second: 147, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.464 [0.000, 1.000],  loss: 3.988206, mae: 10.480824, mean_q: 20.320076, mean_eps: 0.662635\n",
            " 3824/10000: episode: 135, duration: 0.382s, episode steps:  61, steps per second: 160, episode reward: 61.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.525 [0.000, 1.000],  loss: 4.181232, mae: 10.543891, mean_q: 20.421763, mean_eps: 0.658630\n",
            " 3840/10000: episode: 136, duration: 0.105s, episode steps:  16, steps per second: 153, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 4.161122, mae: 10.613029, mean_q: 20.592964, mean_eps: 0.655165\n",
            " 3872/10000: episode: 137, duration: 0.203s, episode steps:  32, steps per second: 158, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.469 [0.000, 1.000],  loss: 4.497742, mae: 10.664354, mean_q: 20.655090, mean_eps: 0.653005\n",
            " 3930/10000: episode: 138, duration: 0.433s, episode steps:  58, steps per second: 134, episode reward: 58.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 4.450081, mae: 10.775808, mean_q: 20.859991, mean_eps: 0.648955\n",
            " 3943/10000: episode: 139, duration: 0.123s, episode steps:  13, steps per second: 106, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.385 [0.000, 1.000],  loss: 5.622691, mae: 10.876887, mean_q: 20.982911, mean_eps: 0.645760\n",
            " 4012/10000: episode: 140, duration: 0.642s, episode steps:  69, steps per second: 107, episode reward: 69.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  loss: 4.897038, mae: 10.933948, mean_q: 21.126335, mean_eps: 0.642070\n",
            " 4082/10000: episode: 141, duration: 0.623s, episode steps:  70, steps per second: 112, episode reward: 70.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 5.038781, mae: 11.036579, mean_q: 21.302074, mean_eps: 0.635815\n",
            " 4145/10000: episode: 142, duration: 0.579s, episode steps:  63, steps per second: 109, episode reward: 63.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  loss: 5.022501, mae: 11.183165, mean_q: 21.631930, mean_eps: 0.629830\n",
            " 4169/10000: episode: 143, duration: 0.218s, episode steps:  24, steps per second: 110, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 5.006510, mae: 11.256693, mean_q: 21.765054, mean_eps: 0.625915\n",
            " 4196/10000: episode: 144, duration: 0.245s, episode steps:  27, steps per second: 110, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  loss: 5.242894, mae: 11.258862, mean_q: 21.786893, mean_eps: 0.623620\n",
            " 4238/10000: episode: 145, duration: 0.266s, episode steps:  42, steps per second: 158, episode reward: 42.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 4.988592, mae: 11.338488, mean_q: 22.043782, mean_eps: 0.620515\n",
            " 4307/10000: episode: 146, duration: 0.429s, episode steps:  69, steps per second: 161, episode reward: 69.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.507 [0.000, 1.000],  loss: 5.592928, mae: 11.447294, mean_q: 22.109498, mean_eps: 0.615520\n",
            " 4397/10000: episode: 147, duration: 0.546s, episode steps:  90, steps per second: 165, episode reward: 90.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  loss: 5.683174, mae: 11.569750, mean_q: 22.440109, mean_eps: 0.608365\n",
            " 4491/10000: episode: 148, duration: 0.570s, episode steps:  94, steps per second: 165, episode reward: 94.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.489 [0.000, 1.000],  loss: 6.022827, mae: 11.752449, mean_q: 22.709928, mean_eps: 0.600085\n",
            " 4520/10000: episode: 149, duration: 0.184s, episode steps:  29, steps per second: 158, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.552 [0.000, 1.000],  loss: 6.887328, mae: 11.860924, mean_q: 22.826769, mean_eps: 0.594550\n",
            " 4547/10000: episode: 150, duration: 0.175s, episode steps:  27, steps per second: 154, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.556 [0.000, 1.000],  loss: 7.204115, mae: 11.904013, mean_q: 22.925026, mean_eps: 0.592030\n",
            " 4583/10000: episode: 151, duration: 0.235s, episode steps:  36, steps per second: 153, episode reward: 36.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 7.743519, mae: 11.886921, mean_q: 22.790772, mean_eps: 0.589195\n",
            " 4628/10000: episode: 152, duration: 0.307s, episode steps:  45, steps per second: 146, episode reward: 45.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.489 [0.000, 1.000],  loss: 5.207756, mae: 11.940587, mean_q: 23.213189, mean_eps: 0.585550\n",
            " 4671/10000: episode: 153, duration: 0.287s, episode steps:  43, steps per second: 150, episode reward: 43.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.512 [0.000, 1.000],  loss: 6.201045, mae: 12.090512, mean_q: 23.405805, mean_eps: 0.581590\n",
            " 4715/10000: episode: 154, duration: 0.275s, episode steps:  44, steps per second: 160, episode reward: 44.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.477 [0.000, 1.000],  loss: 6.280109, mae: 12.127725, mean_q: 23.486346, mean_eps: 0.577675\n",
            " 4759/10000: episode: 155, duration: 0.274s, episode steps:  44, steps per second: 160, episode reward: 44.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.477 [0.000, 1.000],  loss: 6.437316, mae: 12.139120, mean_q: 23.458531, mean_eps: 0.573715\n",
            " 4808/10000: episode: 156, duration: 0.298s, episode steps:  49, steps per second: 165, episode reward: 49.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.449 [0.000, 1.000],  loss: 6.040449, mae: 12.288339, mean_q: 23.876908, mean_eps: 0.569530\n",
            " 4895/10000: episode: 157, duration: 0.546s, episode steps:  87, steps per second: 159, episode reward: 87.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 6.116920, mae: 12.401024, mean_q: 24.009468, mean_eps: 0.563410\n",
            " 4975/10000: episode: 158, duration: 0.492s, episode steps:  80, steps per second: 163, episode reward: 80.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.475 [0.000, 1.000],  loss: 6.740612, mae: 12.504675, mean_q: 24.162854, mean_eps: 0.555895\n",
            " 5021/10000: episode: 159, duration: 0.292s, episode steps:  46, steps per second: 157, episode reward: 46.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.543 [0.000, 1.000],  loss: 5.240893, mae: 12.576935, mean_q: 24.500954, mean_eps: 0.550225\n",
            " 5035/10000: episode: 160, duration: 0.091s, episode steps:  14, steps per second: 154, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.643 [0.000, 1.000],  loss: 5.271648, mae: 12.639980, mean_q: 24.596933, mean_eps: 0.547525\n",
            " 5095/10000: episode: 161, duration: 0.382s, episode steps:  60, steps per second: 157, episode reward: 60.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.450 [0.000, 1.000],  loss: 6.400709, mae: 12.689771, mean_q: 24.619581, mean_eps: 0.544195\n",
            " 5169/10000: episode: 162, duration: 0.451s, episode steps:  74, steps per second: 164, episode reward: 74.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.459 [0.000, 1.000],  loss: 7.241679, mae: 12.871187, mean_q: 24.897342, mean_eps: 0.538165\n",
            " 5240/10000: episode: 163, duration: 0.442s, episode steps:  71, steps per second: 161, episode reward: 71.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.521 [0.000, 1.000],  loss: 5.825573, mae: 12.868838, mean_q: 25.088195, mean_eps: 0.531640\n",
            " 5324/10000: episode: 164, duration: 0.519s, episode steps:  84, steps per second: 162, episode reward: 84.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  loss: 6.061968, mae: 13.074384, mean_q: 25.476827, mean_eps: 0.524665\n",
            " 5372/10000: episode: 165, duration: 0.296s, episode steps:  48, steps per second: 162, episode reward: 48.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.542 [0.000, 1.000],  loss: 7.535422, mae: 13.249267, mean_q: 25.649209, mean_eps: 0.518725\n",
            " 5408/10000: episode: 166, duration: 0.231s, episode steps:  36, steps per second: 156, episode reward: 36.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.528 [0.000, 1.000],  loss: 5.142190, mae: 13.214058, mean_q: 25.879600, mean_eps: 0.514945\n",
            " 5432/10000: episode: 167, duration: 0.161s, episode steps:  24, steps per second: 149, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.542 [0.000, 1.000],  loss: 5.173018, mae: 13.265373, mean_q: 26.105510, mean_eps: 0.512245\n",
            " 5466/10000: episode: 168, duration: 0.210s, episode steps:  34, steps per second: 162, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 6.819650, mae: 13.466268, mean_q: 26.285756, mean_eps: 0.509635\n",
            " 5518/10000: episode: 169, duration: 0.330s, episode steps:  52, steps per second: 158, episode reward: 52.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  loss: 6.205272, mae: 13.400325, mean_q: 26.220992, mean_eps: 0.505765\n",
            " 5561/10000: episode: 170, duration: 0.312s, episode steps:  43, steps per second: 138, episode reward: 43.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.465 [0.000, 1.000],  loss: 6.270833, mae: 13.531694, mean_q: 26.475350, mean_eps: 0.501490\n",
            " 5710/10000: episode: 171, duration: 0.939s, episode steps: 149, steps per second: 159, episode reward: 149.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 6.606125, mae: 13.696850, mean_q: 26.758141, mean_eps: 0.492850\n",
            " 5841/10000: episode: 172, duration: 1.021s, episode steps: 131, steps per second: 128, episode reward: 131.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  loss: 6.588732, mae: 13.949272, mean_q: 27.251434, mean_eps: 0.480250\n",
            " 5973/10000: episode: 173, duration: 1.199s, episode steps: 132, steps per second: 110, episode reward: 132.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  loss: 5.814390, mae: 14.174769, mean_q: 27.803214, mean_eps: 0.468415\n",
            " 6075/10000: episode: 174, duration: 0.853s, episode steps: 102, steps per second: 120, episode reward: 102.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  loss: 8.073283, mae: 14.399018, mean_q: 28.014320, mean_eps: 0.457885\n",
            " 6181/10000: episode: 175, duration: 0.672s, episode steps: 106, steps per second: 158, episode reward: 106.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.481 [0.000, 1.000],  loss: 5.985004, mae: 14.534043, mean_q: 28.476410, mean_eps: 0.448525\n",
            " 6259/10000: episode: 176, duration: 0.507s, episode steps:  78, steps per second: 154, episode reward: 78.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  loss: 5.887983, mae: 14.660865, mean_q: 28.816017, mean_eps: 0.440245\n",
            " 6346/10000: episode: 177, duration: 0.570s, episode steps:  87, steps per second: 153, episode reward: 87.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  loss: 7.833935, mae: 14.891567, mean_q: 29.045141, mean_eps: 0.432820\n",
            " 6388/10000: episode: 178, duration: 0.272s, episode steps:  42, steps per second: 154, episode reward: 42.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 7.424529, mae: 14.927795, mean_q: 29.205005, mean_eps: 0.427015\n",
            " 6452/10000: episode: 179, duration: 0.397s, episode steps:  64, steps per second: 161, episode reward: 64.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.547 [0.000, 1.000],  loss: 8.138479, mae: 15.023498, mean_q: 29.276346, mean_eps: 0.422245\n",
            " 6559/10000: episode: 180, duration: 0.669s, episode steps: 107, steps per second: 160, episode reward: 107.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 7.755519, mae: 15.129311, mean_q: 29.549391, mean_eps: 0.414550\n",
            " 6719/10000: episode: 181, duration: 0.994s, episode steps: 160, steps per second: 161, episode reward: 160.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 6.973113, mae: 15.264155, mean_q: 29.958006, mean_eps: 0.402535\n",
            " 6815/10000: episode: 182, duration: 0.589s, episode steps:  96, steps per second: 163, episode reward: 96.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.458 [0.000, 1.000],  loss: 6.804826, mae: 15.497578, mean_q: 30.459868, mean_eps: 0.391015\n",
            " 6875/10000: episode: 183, duration: 0.377s, episode steps:  60, steps per second: 159, episode reward: 60.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.550 [0.000, 1.000],  loss: 8.032098, mae: 15.648412, mean_q: 30.587273, mean_eps: 0.383995\n",
            " 6941/10000: episode: 184, duration: 0.413s, episode steps:  66, steps per second: 160, episode reward: 66.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.470 [0.000, 1.000],  loss: 5.259030, mae: 15.717827, mean_q: 30.949790, mean_eps: 0.378325\n",
            " 7017/10000: episode: 185, duration: 0.477s, episode steps:  76, steps per second: 159, episode reward: 76.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.553 [0.000, 1.000],  loss: 6.632019, mae: 15.924719, mean_q: 31.338865, mean_eps: 0.371935\n",
            " 7145/10000: episode: 186, duration: 0.797s, episode steps: 128, steps per second: 161, episode reward: 128.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.531 [0.000, 1.000],  loss: 7.137979, mae: 16.032252, mean_q: 31.526766, mean_eps: 0.362755\n",
            " 7247/10000: episode: 187, duration: 0.625s, episode steps: 102, steps per second: 163, episode reward: 102.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.461 [0.000, 1.000],  loss: 7.585434, mae: 16.275194, mean_q: 31.945660, mean_eps: 0.352405\n",
            " 7342/10000: episode: 188, duration: 0.588s, episode steps:  95, steps per second: 162, episode reward: 95.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.537 [0.000, 1.000],  loss: 8.304105, mae: 16.403012, mean_q: 32.109486, mean_eps: 0.343540\n",
            " 7421/10000: episode: 189, duration: 0.496s, episode steps:  79, steps per second: 159, episode reward: 79.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.544 [0.000, 1.000],  loss: 6.526357, mae: 16.514322, mean_q: 32.527655, mean_eps: 0.335710\n",
            " 7520/10000: episode: 190, duration: 0.628s, episode steps:  99, steps per second: 158, episode reward: 99.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.475 [0.000, 1.000],  loss: 6.712241, mae: 16.701973, mean_q: 32.893938, mean_eps: 0.327700\n",
            " 7652/10000: episode: 191, duration: 0.883s, episode steps: 132, steps per second: 150, episode reward: 132.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.530 [0.000, 1.000],  loss: 7.617925, mae: 16.865038, mean_q: 33.166786, mean_eps: 0.317305\n",
            " 7740/10000: episode: 192, duration: 0.803s, episode steps:  88, steps per second: 110, episode reward: 88.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.534 [0.000, 1.000],  loss: 10.399722, mae: 17.075352, mean_q: 33.380787, mean_eps: 0.307405\n",
            " 7940/10000: episode: 193, duration: 1.711s, episode steps: 200, steps per second: 117, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  loss: 8.297679, mae: 17.208194, mean_q: 33.863883, mean_eps: 0.294445\n",
            " 8140/10000: episode: 194, duration: 1.245s, episode steps: 200, steps per second: 161, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 8.104845, mae: 17.543427, mean_q: 34.611236, mean_eps: 0.276445\n",
            " 8260/10000: episode: 195, duration: 0.798s, episode steps: 120, steps per second: 150, episode reward: 120.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  loss: 7.955384, mae: 17.799128, mean_q: 35.203818, mean_eps: 0.262045\n",
            " 8379/10000: episode: 196, duration: 0.790s, episode steps: 119, steps per second: 151, episode reward: 119.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: 9.152961, mae: 17.988192, mean_q: 35.522949, mean_eps: 0.251290\n",
            " 8579/10000: episode: 197, duration: 1.319s, episode steps: 200, steps per second: 152, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  loss: 8.978890, mae: 18.194173, mean_q: 35.929717, mean_eps: 0.236935\n",
            " 8779/10000: episode: 198, duration: 1.300s, episode steps: 200, steps per second: 154, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 8.436152, mae: 18.583168, mean_q: 36.816034, mean_eps: 0.218935\n",
            " 8901/10000: episode: 199, duration: 0.775s, episode steps: 122, steps per second: 157, episode reward: 122.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.459 [0.000, 1.000],  loss: 7.511666, mae: 18.849718, mean_q: 37.375441, mean_eps: 0.204445\n",
            " 9101/10000: episode: 200, duration: 1.228s, episode steps: 200, steps per second: 163, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  loss: 8.619036, mae: 19.121039, mean_q: 37.914455, mean_eps: 0.189955\n",
            " 9301/10000: episode: 201, duration: 1.230s, episode steps: 200, steps per second: 163, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.510 [0.000, 1.000],  loss: 10.448862, mae: 19.470644, mean_q: 38.412657, mean_eps: 0.171955\n",
            " 9501/10000: episode: 202, duration: 1.248s, episode steps: 200, steps per second: 160, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.470 [0.000, 1.000],  loss: 9.698046, mae: 19.678050, mean_q: 38.915258, mean_eps: 0.153955\n",
            " 9701/10000: episode: 203, duration: 1.757s, episode steps: 200, steps per second: 114, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 10.194131, mae: 19.965382, mean_q: 39.492647, mean_eps: 0.135955\n",
            " 9901/10000: episode: 204, duration: 1.454s, episode steps: 200, steps per second: 138, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  loss: 9.159067, mae: 20.202337, mean_q: 40.008070, mean_eps: 0.117955\n",
            "done, took 68.630 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# summarize the history for number  of episode steps\n",
        "plt.plot(history.history['nb_episode_steps'])\n",
        "plt.ylabel('nb_episode_steps')\n",
        "plt.xlabel('episodes')\n",
        "plt.show()\n",
        "\n",
        "dqn.test(env, nb_episodes=20, visualize=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 661
        },
        "id": "bkKOscyvlbDf",
        "outputId": "5e18f6cf-a5da-4bbf-f24a-e7bc0bf73e99"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing for 20 episodes ...\n",
            "Episode 1: reward: 200.000, steps: 200\n",
            "Episode 2: reward: 200.000, steps: 200\n",
            "Episode 3: reward: 200.000, steps: 200\n",
            "Episode 4: reward: 200.000, steps: 200\n",
            "Episode 5: reward: 200.000, steps: 200\n",
            "Episode 6: reward: 200.000, steps: 200\n",
            "Episode 7: reward: 200.000, steps: 200\n",
            "Episode 8: reward: 200.000, steps: 200\n",
            "Episode 9: reward: 200.000, steps: 200\n",
            "Episode 10: reward: 200.000, steps: 200\n",
            "Episode 11: reward: 200.000, steps: 200\n",
            "Episode 12: reward: 200.000, steps: 200\n",
            "Episode 13: reward: 200.000, steps: 200\n",
            "Episode 14: reward: 200.000, steps: 200\n",
            "Episode 15: reward: 200.000, steps: 200\n",
            "Episode 16: reward: 200.000, steps: 200\n",
            "Episode 17: reward: 200.000, steps: 200\n",
            "Episode 18: reward: 200.000, steps: 200\n",
            "Episode 19: reward: 200.000, steps: 200\n",
            "Episode 20: reward: 200.000, steps: 200\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f20182492b0>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    }
  ]
}